#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[hidelinks]{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
How the Neural Network Learns
\end_layout

\begin_layout Author
Qiang Feng
\end_layout

\begin_layout Standard
In order for the neural network to learn, we need a learning algorithm so
 that the weights and biases can be updated to improve the output.
 One of the most popular algorithms is Back Propagation.
\end_layout

\begin_layout Section
Back-Propagation
\end_layout

\begin_layout Standard
Consider a cost function 
\begin_inset Formula $J(\theta)$
\end_inset

 defined to be the error (i.e.
 the difference between the expected and the actual output), where 
\begin_inset Formula $\theta$
\end_inset

 is the parameters of the neural network (weights, biases etc).
\end_layout

\begin_layout Standard
During training, training data is input into the neural network, and then
 the output is compared with the expected output.
 We then use 
\begin_inset Formula $J$
\end_inset

 to determine the error of each output neuron in the last layer (output
 layer) of the neural network.
 Once this error has been calculated we can also calculate the error of
 the previous layer as we have the error associated with each node and the
 weight of each arc to that node.
 This can be repeated for each layer of neurons in the network and so the
 error is "back-propagated" through the network.
\end_layout

\begin_layout Standard
We then have an error term for each neuron, and a weight for each input
 into that neuron, so we can then compute the partial derivative 
\begin_inset Formula $\frac{\partial J}{\partial w}$
\end_inset

 where 
\begin_inset Formula $w$
\end_inset

 is the weight of the output arc from that particular node.
 This allows for the computation of the gradient of the cost function 
\begin_inset Formula $J$
\end_inset

, which is then allows the optimisation method (such as gradient descent,
 which will be discussed later on) to update the weight 
\begin_inset Formula $w$
\end_inset

 for that output arc 
\begin_inset CommandInset citation
LatexCommand cite
key "NNDL"

\end_inset

.
\end_layout

\begin_layout Standard
Note that in order to work out 
\begin_inset Formula $\frac{\partial J}{\partial w}$
\end_inset

, 
\begin_inset Formula $J$
\end_inset

 must be differentiable, hence the activation function must also be differentiab
le 
\begin_inset CommandInset citation
LatexCommand cite
key "RojasNN"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintAll"
bibfiles "learning"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
