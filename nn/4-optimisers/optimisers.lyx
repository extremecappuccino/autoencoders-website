#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[hidelinks]{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "" "default"
\font_sans "" "default"
\font_typewriter "" "default"
\font_math "" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Optimisers
\end_layout

\begin_layout Author
Laurence Squires
\end_layout

\begin_layout Section
Gradient Descent
\end_layout

\begin_layout Standard
Gradient descent is the general purpose way to minimize a cost function
 
\begin_inset Formula $J(\text{𝜃})$
\end_inset

 with 
\begin_inset Formula $\text{𝜃}$
\end_inset

 being the parameters of the model (such as weights and biases).
 The learning rate 
\begin_inset Formula $\text{𝜂}$
\end_inset

 determines the speed at which it learns, and the step size we take at each
 update interval, too slow a learning rate and convergence to the minimum
 takes too long, too high the learning rate and the parameters will not
 reach the minimum as they will not get fine-tuned.
\end_layout

\begin_layout Standard
There are three main ways to do gradient descent, and choosing the correct
 one depends on the amount of data and the trade-off the user makes between
 speed and accuracy.
\end_layout

\begin_layout Subsection

\series bold
Batch gradient descent
\end_layout

\begin_layout Standard
This is the most basic type of gradient descent and works well for small
 data sets.
 It computes the gradient of the cost function with respect to the parameters
 of the model for the entire model.
 This version of gradient descent is very slow and very expensive as the
 entire data set needs to be processed for one update.
 Furthermore, using this version on modern machine learning frameworks doesn't
 support the adding of new examples (on-the-fly learning) as the model is
 training, the reason for this is because the input is the entire data set
 the model is designed to take inputs of that size, and any change to the
 size of the inputs requires reloading of the model.
\end_layout

\begin_layout Subsection
Stochastic gradient descent
\end_layout

\begin_layout Standard
Stochastic gradient descent (SGD) is gradient descent performed for each
 input, so each update interval occurs after one input has been processed
 through the network.
 This is a faster way of training the model than batch gradient descent
 as batch will perform redundant computations for each update cycle as it
 computes the whole data set and so the update is not granted to affect
 the performance of a single input.
 In addition, SGD allows for on the fly learning as the input size isn't
 changed by adding more examples to the data set.
 The downside is that while training the model the cost function will fluctuate
 wildly as each input is different from each other.
 This is shown in the graph below.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename res/Stogra.png
	width 45text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Wild fluctuation in the SGD cost function while training 
\begin_inset CommandInset citation
LatexCommand cite
key "sgdTraining"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Mini-batch gradient descent
\end_layout

\begin_layout Standard
Mini-batch gradient descent is a mixture of the two previous methods.
 Instead of training the network on the entire data set, the data set is
 partitioned into batches of fixed size and then fed into the model for
 training.
 This enables it the converge fast like SGD but can also be computed faster,
 most modern day machine learning libraries take advantage of the GPU and
 its ability to perform large matrix calculations quickly, by fixing the
 input size bigger than 1 (such as in SGD) this enables the entire input
 to be generalized and stored in memory as a tensor (higher dimensional
 matrix) and then computed through the model very quickly by the GPU.
 This method then utilizes the best of the previous methods and is normally
 the chosen method for practical machine learning.
\end_layout

\begin_layout Section
Optimisers
\end_layout

\begin_layout Standard
For all the previous examples we have used the simplest form of the gradient
 descent algorithm.
 Simply calculating the gradient of the cost function, multiplying it by
 a constant learning rate and then subtracting it from the current the parameter
s so our parameters take a tiny step in the direction we hope minimises
 the cost function.
 This is shown in the update algorithm below.
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Formula $\text{𝜃}=\text{𝜃}-\text{\text{𝜂}∇J(𝜃)}$
\end_inset


\end_layout

\begin_layout Subsection
Momentum
\end_layout

\begin_layout Standard
The normal gradient descent algorithm has difficulty dealing with ravines,
 or places with much sharper surface curves in one dimension.
 This causes our parameters to constantly 
\begin_inset Quotes eld
\end_inset

roll
\begin_inset Quotes erd
\end_inset

 from one side of the ravine to the other as the gradient in these places
 is much larger than the gradient leading us to the optimal solution.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename res/without_momentum.gif
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The update steps without momentum
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Momentum is a method that helps deal with this problem by simply adding
 a momentum to the update step.
 It achieves this by adding a fraction of the past update vector to the
 current vector.
 
\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Formula $v_{t}=0.9*v_{t-1}+\text{𝜂}\text{∇}J(\text{𝜃})$
\end_inset


\end_layout

\begin_layout Standard
\noindent
\align center
\begin_inset Formula $\theta=\theta-v_{t}$
\end_inset


\end_layout

\begin_layout Standard
Therefore, our updates accumulate momentum in the correct direction and
 reach the optimal solution much faster, as shown in this diagram.
 This is similar to how a ball rolls down a hill or ravine, accumulating
 velocity in the direction towards the lowest point.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename res/with_momentum.gif
	width 40text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The update step with momentum
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "optimisers"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
