#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[hidelinks]{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Regularisation
\end_layout

\begin_layout Author
Qiang Feng
\end_layout

\begin_layout Standard
Over fitting is a problem where the neural network performs very good when
 training data is used, and performs poorly when unseen data is input into
 the network (the actual output is hugely different to the expected output)
 
\begin_inset CommandInset citation
LatexCommand cite
key "NikhilBudumaDL"

\end_inset

 - i.e.
 the network is trying to match to the training data exactly.
 An illustration of this is shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:overfitting"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename res/overfitting.png
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example where over fitting has occurred 
\begin_inset CommandInset label
LatexCommand label
name "fig:overfitting"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The complex blue line is over fitted to the training data set, whereas the
 red line shows what an ideal match 
\emph on
should
\emph default
 be.
\end_layout

\begin_layout Standard
Regularisation is used to prevent over fitting.
 There are different regularisation techniques such as L1 and L2, however
 we will be looking at the Dropout technique in particular.
\end_layout

\begin_layout Section
Dropout
\end_layout

\begin_layout Standard
Dropout is a technique where certain neurons in a neural network are temporarily
 disabled during the training phase.
 This prevents the neural network from being dependent upon certain neurons.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename res/dropout.png
	width 70text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A neural network before and after applying dropout 
\begin_inset CommandInset label
LatexCommand label
name "fig:dropout"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Dropout"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Implementation
\end_layout

\begin_layout Itemize
During 
\series bold
training
\series default
 time: each neuron in all hidden layers has a probability 
\begin_inset Formula $p$
\end_inset

 which is determines to be the probability of it being enabled
\end_layout

\begin_layout Itemize
During 
\series bold
use
\series default
: every neuron (in hidden layers) is always enabled, but all the output
 arc weights are multiplied with with 
\begin_inset Formula $p$
\end_inset

 (to ensure that the neuron outputs during use are the same as during training)
 
\begin_inset CommandInset citation
LatexCommand cite
key "Dropout"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "regularisation"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
