<!DOCTYPE html>
<html>
<head>
  <title>Autoencoders: Artificial Neurons</title>
  <link href="https://fonts.googleapis.com/css?family=Raleway:300|Source+Code+Pro" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="../css/normalise.css">
  <link rel="stylesheet" type="text/css" href="../css/global.css">
  <link rel="stylesheet" type="text/css" href="../css/content.css">
</head>
<body>
<!-- Menu -->
  <span class="hamburger"></span>
  <nav class="hidden">
    <div class="brand">
      <a href="/"><img src="../img/logo.svg"> AUTOENCODERS</a>
    </div>
    <ul>
      <li><a href="#" content="neural-networks">Neural Networks</a></li>
      <li class="hidden" content="neural-networks">
        <ul>
          <li><a href="/neural-networks">Introduction</a></li>
          <li><a href="/neural-networks/neurons.html">Artificial Neurons</a></li>
          <li><a href="/neural-networks/learning.html">Learning</a></li>
          <li><a href="/neural-networks/regularisation.html">Regularisation</a></li>
          <li><a href="/neural-networks/optimisers.html">Optimisers</a></li>
        </ul>
      </li>
      <li><a href="#" content="autoencoders">Autoencoders</a></li>
      <li class="hidden" content="autoencoders">
        <ul>
          <li><a href="/autoencoders">Introduction</a></li>
          <li><a href="/autoencoders/history.html">History</a></li>
          <li><a href="/autoencoders/denoising.html">Autoencoders for Denoising</a></li>
          <li><a href="/autoencoders/variational.html">Variational Autoencoders</a></li>
          <li><a href="/autoencoders/sparse.html">Sparse Autoencoders</a></li>
        </ul>
      </li>
      <li><a href="#" content="uses-autoencoders">Uses of Autoencoders</a></li>
      <li class="hidden" content="uses-autoencoders">
        <ul>
          <li><a href="/autoencoders/ip.html">Image Processing</a></li>
          <li><a href="/autoencoders/pretraining.html">Pretraining</a></li>
        </ul>
      </li>
      <li class="last"><a href="#" content="nlp">Autoencoders in NLP</a></li>
      <li class="hidden last" content="nlp">
        <ul>
          <li><a href="/nlp">Introduction</a></li>
          <li><a href="/nlp/word-phrase.html">Word and Phrase Representations</a></li>
          <li><a href="/nlp/bilingual.html">Bilingual Phrase Representations</a></li>
          <li><a href="/nlp/recursive.html">Recursive Autoencoders</a></li>
          <li><a href="/nlp/sentiment.html">Sentiment Analysis</a></li>
          <li><a href="/nlp/paraphrase.html">Paraphrase Detection</a></li>
        </ul>
      </li>
    </ul>
    <div class="close"></div>
  </nav>
  <div class="overlay hidden"></div>
  
  <div class="container">
    <!-- BEGIN: LyX Generated HTML -->
    <h2 class="titleHead">Artificial Neurons</h2>
    <h3 class="sectionHead"><span class="titlemark">1</span> <a id="x1-10001"></a>Structure</h3><!--l. 34-->
    <p class="noindent">An artificial neuron in a neural network is essentially a mathematical function. It receives inputs from neurons in a previous layer (or, in the case of the input layer, directly from a file or outside signal). It then performs the summation:</p>
    <center class="math-display">
      $
        v=\sum{w_ix_i}
      $
    </center><!--l. 40-->
    <p class="nopar">where $w_i$ is the weight assigned to the arc the information is travelling through and $x_i$ is the information passed through the arc $i$. A bias $b$ is added on to this sum:</p>
    <center class="par-math-display">
      $
        v=\sum{w_ix_i}+b
      $
    </center><!--l. 47-->
    <p class="nopar"><!--l. 50--></p>
    <p class="noindent">After the bias is added, it is then passed through an activation function $f$:</p>
    <center class="par-math-display">
      $
        v=f(\sum{w_ix_i}+b)
      $
    </center><!--l. 55-->
    <p class="nopar"><!--l. 58--></p>
    <p class="noindent">The result <span class="cmmi-10">v</span> is then output from this neuron. <!--l. 61--></p>
    <p class="noindent"></p>
    <h4 class="subsectionHead"><span class="titlemark">1.1</span> <a id="x1-20001.1"></a>Information</h4><!--l. 63-->
    <p class="noindent">All data passed into the neural network need to be in the form of vectors, all of the same dimension <span class="cite">[<a href="#XDataForDeepLearningDL4J">1</a>]</span>. This is so that mathematical operations such as the one above can be performed on the data. <!--l. 69--></p>
    <p class="noindent"></p>
    <h4 class="subsectionHead"><span class="titlemark">1.2</span> <a id="x1-30001.2"></a>Weights</h4><!--l. 71-->
    <p class="noindent">A weight is a number associated with an arc which carries information from one neuron to another. It is used to represent the fact that certain pairs of neurons have a stronger connection (which is represented using a weight of higher absolute value), and others have very weak to no connection at all (represented using a weight $|w| \rightarrow 0$). <!--l. 78--></p>
    <p class="noindent"></p>
    <h4 class="subsectionHead"><span class="titlemark">1.3</span> <a id="x1-40001.3"></a>Biases</h4><!--l. 80-->
    <p class="noindent">A bias a constant (for a particular neuron) that is added onto the summation, before it is passed to the activation function. Essentially, it is a measure of how easy it is for the neuron to activate - the larger the bias, the easier it is to fire the neuron at a high rate) <span class="cite">[<a href="#XNNDLC1">2</a>]</span>. <!--l. 87--></p>
    <p class="noindent"></p>
    <h3 class="sectionHead"><span class="titlemark">2</span> <a id="x1-50002"></a>Activation Function</h3><!--l. 89-->
    <p class="noindent">The purpose of the activation function is to decide whether or not the inputs are significant enough to activate the neuron (i.e. whether or not to activate the neuron, and also how active this neuron should be) <span class="cite">[<a href="#XCS231nCNN">3</a>]</span>. <!--l. 95--></p>
    <p class="noindent"></p>
    <h4 class="subsectionHead"><span class="titlemark">2.1</span> <a id="x1-60002.1"></a>Unit Step</h4><!--l. 97-->
    <p class="noindent">The simplest activation function is the step function. When the sum of the inputs with weights is greater than or equal 0, it returns 1 (equivalent of the neuron firing). Otherwise it returns 0 (the neuron remains inactive.</p>
    <center class="par-math-display">
      $
        f(x)=\begin{cases}
                1 & x\geq0\\
                0 & x<0
              \end{cases}
      $
    </center><!--l. 107-->
    <p class="nopar"><!--l. 110--></p>
    <p class="noindent"></p>
    <div class="center">
      <!--l. 110-->
      <p class="noindent"><!--l. 111--></p>
      <p class="noindent"><img class="smaller" alt="PIC" src="../img/neural-networks/graph-step.png"></p>
    </div><!--l. 115-->
    <p class="noindent"></p>
    <h4 class="subsectionHead"><span class="titlemark">2.2</span> <a id="x1-70002.2"></a>Sigmoid</h4><!--l. 117-->
    <p class="noindent">The Sigmoid activation function has a smooth transition from the 0 to 1, so small changes in weights and biases result in small changes in the output value. This particular property of this function makes it useful for training the neural network. Using a learning algorithm such as Back Propagation, we can make small changes in the weights of the arcs connecting to this neuron to get small changes in the output, in order to reduce the error (the difference between the expected and actual output).</p>
    <center class="par-math-display">
      $
        f(x)=\frac{1}{1+e^{-x}}
      $
    </center><!--l. 128-->
    <p class="nopar"><!--l. 131--></p>
    <p class="noindent"></p>
    <div class="center">
      <!--l. 131-->
      <p class="noindent"><!--l. 132--></p>
      <p class="noindent"><img class="smaller" alt="PIC" src="../img/neural-networks/graph-sigmoid.png"></p>
    </div><!--l. 135-->
    <p class="noindent">If the step function was used, the small changes in biases and weights would not guarantee the change in output to be small, so it could affect the entire network drastically <span class="cite">[<a href="#XNNDLC1">2</a>]</span>. <!--l. 140--></p>
    <p class="noindent"></p>
    <h4 class="subsectionHead"><span class="titlemark">2.3</span> <a id="x1-80002.3"></a>Tanh</h4><!--l. 142-->
    <p class="noindent">This function is very similar to the Sigmoid activation function. However, it the range of this function is $[-1,1]$ whereas the range of the Sigmoid function is $[0,1]$.</p>
    <center class="par-math-display">
      $
        f(x)=\tanh(x)
      $
    </center><!--l. 148-->
    <p class="nopar"><!--l. 151--></p>
    <p class="noindent"></p>
    <div class="center">
      <!--l. 151-->
      <p class="noindent"><!--l. 152--></p>
      <p class="noindent"><img class="smaller" alt="PIC" src="../img/neural-networks/graph-tanh.png"></p>
    </div><!--l. 156-->
    <p class="noindent"></p>
    <h4 class="subsectionHead"><span class="titlemark">2.4</span> <a id="x1-90002.4"></a>ReLU (Rectified Linear Unit)</h4><!--l. 158-->
    <p class="noindent">This was the most popular non linear activation function in 2015 <span class="cite">[<a href="#XNatureDL">4</a>]</span>. This is due to multi-layer neural networks (i.e. multiple hidden layers) learning much faster than when using smooth activation functions such as Tanh and Sigmoid.</p>
    <center class="par-math-display">
      $
        f(x)=\max(0,x)
      $
    </center><!--l. 165-->
    <p class="nopar"><!--l. 168--></p>
    <p class="noindent"></p>
    <div class="center">
      <!--l. 168-->
      <p class="noindent"><!--l. 169--></p>
      <p class="noindent"><img class="smaller" alt="PIC" src="../img/neural-networks/graph-relu.png"></p>
    </div><!--l. 1-->
    <p class="noindent"></p>
    <!-- END: LyX Generated HTML -->
    <div class="navigation">
      <a class="button" href="neural-networks/index.html">Previous</a>
      <a class="button dark next" href="neural-networks/learning.html">Next</a>
      <div class="clearfix"></div>
    </div>
  </div>
  <div class="well">
    <div class="container">
      <!-- BEGIN: LyX Generated HTML -->
      <h3 class="likesectionHead"><a id="x1-100002.4"></a>References</h3><!--l. 1-->
      <p class="noindent"></p>
      <div class="thebibliography">
        <p class="bibitem"><span class="biblabel">[1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XDataForDeepLearningDL4J"></a>Skymind. The Data You Need For Deep Learning - Deeplearning4j;. Available from: <a class="url" href="https://deeplearning4j.org/data-for-deep-learning.html"><span class="cmtt-10">https://deeplearning4j.org/data-_for-_deep-_learning.html</span></a>.</p>
        <p class="bibitem"><span class="biblabel">[2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XNNDLC1"></a>Nielsen M. Using neural nets to recognize handwritten digits;. Available from: <a class="url" href="http://neuralnetworksanddeeplearning.com/chap1.html"><span class="cmtt-10">http://neuralnetworksanddeeplearning.com/chap1.html</span></a>.</p>
        <p class="bibitem"><span class="biblabel">[3]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XCS231nCNN"></a>karpathy@cs[dot]stanford[dot]edu. CS231n Convolutional Neural Networks for Visual Recognition;. Available from: <a class="url" href="https://cs231n.github.io/neural-networks-1"><span class="cmtt-10">https://cs231n.github.io/neural-_networks-_1</span></a>.</p>
        <p class="bibitem"><span class="biblabel">[4]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XNatureDL"></a>LeCun Y, Bengio Y, Hinton G. Deep Learning;. Available from: <a class="url" href="http://www.nature.com/nature/journal/v521/n7553/pdf/nature14539.pdf"><span class="cmtt-10">http://www.nature.com/nature/journal/v521/n7553/pdf/nature14539.pdf</span></a>.</p>
      </div>
      <!-- END: LyX Generated HTML -->
    </div>
  </div>
  <script type="text/javascript" src="../js/jquery.min.js"></script>
  <script type="text/javascript" src="../js/global.js"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</body>
</html>