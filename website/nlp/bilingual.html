<!DOCTYPE html>
<html>
<head>
  <title>Autoencoders: Word and Phrase Representations</title>
  <link href="https://fonts.googleapis.com/css?family=Raleway:300|Source+Code+Pro" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="../css/normalise.css">
  <link rel="stylesheet" type="text/css" href="../css/global.css">
  <link rel="stylesheet" type="text/css" href="../css/content.css">
</head>
<body>
  <!-- Menu -->
  <span class="hamburger"></span>
  <nav class="hidden">
    <div class="brand">
      <a href="/"><img src="../img/logo.svg"> AUTOENCODERS</a>
    </div>
    <ul>
      <li><a href="#" content="neural-networks">Neural Networks</a></li>
      <li class="hidden" content="neural-networks">
        <ul>
          <li><a href="/neural-networks">Introduction</a></li>
          <li><a href="/neural-networks/neurons.html">Artificial Neurons</a></li>
          <li><a href="/neural-networks/learning.html">Learning</a></li>
          <li><a href="/neural-networks/regularisation.html">Regularisation</a></li>
          <li><a href="/neural-networks/optimisers.html">Optimisers</a></li>
        </ul>
      </li>
      <li><a href="#" content="autoencoders">Autoencoders</a></li>
      <li class="hidden" content="autoencoders">
        <ul>
          <li><a href="/autoencoders">Introduction</a></li>
          <li><a href="/autoencoders/denoising.html">Autoencoders for Denoising</a></li>
          <li><a href="/autoencoders/variational.html">Variational Autoencoders</a></li>
          <li><a href="/autoencoders/sparse.html">Sparse Autoencoders</a></li>
        </ul>
      </li>
      <li><a href="#" content="uses-autoencoders">Uses of Autoencoders</a></li>
      <li class="hidden" content="uses-autoencoders">
        <ul>
          <li><a href="/autoencoders/ip.html">Image Processing</a></li>
          <li><a href="/autoencoders/denoising.html#pre-training">Pretraining</a></li>
        </ul>
      </li>
      <li class="last"><a href="#" content="nlp">Autoencoders in NLP</a></li>
      <li class="hidden last" content="nlp">
        <ul>
          <li><a href="/nlp">Introduction</a></li>
          <li><a href="/nlp/word-phrase.html">Word and Phrase Representations</a></li>
          <li><a href="/nlp/bilingual.html">Bilingual Phrase Representations</a></li>
          <li><a href="/nlp/recursive.html">Recursive Autoencoders</a></li>
          <li><a href="/nlp/sentiment.html">Sentiment Analysis</a></li>
          <li><a href="/nlp/paraphrase.html">Paraphrase Detection</a></li>
        </ul>
      </li>
    </ul>
    <div class="close"></div>
  </nav>
  <div class="overlay hidden"></div>

  <div class="container">
    <!-- BEGIN: LyX Generated HTML -->
    <h2 class="titleHead">Bilingual Representations</h2>
    We will discuss another method for learning representations of phrases (namely the bag-of-words approach), and see how this can be applied to bilingual data where sentences are aligned.
    <h3 class="sectionHead"><span class="titlemark">1</span> <a id="x1-10001"></a>Bag-of-Words</h3><!--l. 35-->
    <p class="noindent"></p>
    <hr class="figure">
    <div class="figure">
      <a id="x1-1001r1"></a> <!--l. 37-->
      <p class="noindent"><img class="smaller" alt="PIC" src="../img/nlp/bow.png"><br></p>
      <div class="caption">
        <span class="id">Figure&#x00A0;1:</span> <span class="content">Demonstration of the bag-of-words approach and how a bag-of-words can be used to generate a vector representation for a phrase.</span>
      </div><!--tex4ht:label?: x1-1001r1 -->
      <!--l. 42-->
      <p class="noindent"></p>
    </div>
    <hr class="endfigure">
    <!--l. 45-->
    <p class="indent">We assume a method for representing words in a vector in $\mathbb{&#x211D;}^D$ exists. Methods for achieving this are discussed on other pages. We also assume a vocabulary of words, size $V$ . <!--l. 49--></p>
    <p class="indent">Given a sentence or phrase, we produce a representation by a simple additive method, which is as follows:</p>
    <ol class="enumerate1">
      <li class="enumerate" id="x1-1003x1">For each word in our vocabulary, we calculate its vector representation in $\mathbb{&#x211D;}^D$. We then define a matrix, say $\mathbf{W}$ of dimensions $D$ <span class="cmsy-10">&#x00D7;</span> $V$ . The matrix $\mathbf{W}$ simply has the vector representation for each of the $V$ vocabulary words as its columns.</li>
      <li class="enumerate" id="x1-1005x2">We start with a bag-of-words representation $\mathbf{x}$, such that each element in $\mathbf{x}$ is an index referencing a word in the vocabulary of $V$ words. Note that in a bag-of-words representation, the order of words in the sentence is completely ignored.</li>
      <li class="enumerate" id="x1-1007x3">The encoder function sums over each colum of $\mathbf{W}$ referring to a word in the given sentence or phrase, giving a result, which we will call $&#x03D5;(x)$. The decoder function is trained using a loss function that measures how well we are able to reconstruct $\mathbf{x}$. <span class="cite">[<a href="#XFergus">1</a>]</span></li>
    </ol><!--l. 67-->
    <p class="noindent"><span class="paragraphHead"><a id="x1-20001"></a><span class="cmbx-10">Concrete Implementation</span></span> In practice, there are many ways to implement a bag-of-words approach to learning phrase representations using autoencoders. We will now discuss a concrete example:</p>
    <ol class="enumerate1">
      <li class="enumerate" id="x1-2002x1">Using the bag-of-words representation $\mathbf{x}$, we define $\mathbf{v}$ in $\mathbb{&#x211D;}^V$ , whereby $\mathbf{v}$ is simply a binary vector, such that each $v_i$ is 1 if the phrase contains the $i$th word of the vocabulary, or 0 otherwise.</li>
      <li class="enumerate" id="x1-2004x2">It is now simple to find the encoder representation, $&#x03D5;(x)$ by multiplication of $\mathbf{v}$ with $\mathbf{W}$:
        <center class="math-display">
          $\phi\left(\mathbf{x}\right)=f\left(\mathbf{Wv}+\mathbf{b}\right)$
        </center><!--l. 81-->
        <p class="nopar">where $\mathbf{b}$ is a bias vector and $f$ is a sigmoidal function, applied element wise to $\mathbf{Wx}$ + $\mathbf{b}$.</p>
      </li>
      <li class="enumerate" id="x1-2006x3">The decoder attempts to reconstruct $\mathbf{v}$ from $&#x03D5;(x)$:
        <center class="math-display">
          $\mathbf{\hat{v}}=g\left(\mathbf{W}'\phi\left(\mathbf{x}\right)+\mathbf{b}'\right)$
        </center><!--l. 87-->
        <p class="nopar"></p>
      </li>
      <li class="enumerate" id="x1-2008x4">Training attempts to minimise the difference between $\mathbf{v}$ and its reconstruction. This is done by using a cross-entropy loss function. <span class="cite">[<a href="#XNielsen2017">2</a>]</span></li>
    </ol><!--l. 94-->
    <p class="noindent"></p>
    <h3 class="sectionHead"><span class="titlemark">2</span> <a id="x1-30002"></a>Bilingual Autoencoders</h3><!--l. 96-->
    <p class="noindent"></p>
    <hr class="figure">
    <div class="figure">
      <a id="x1-3001r2"></a> <!--l. 98-->
      <p class="noindent"><img class="smaller" alt="PIC" src="../img/nlp/bae.png"><br></p>
      <div class="caption">
        <span class="id">Figure&#x00A0;2:</span> <span class="content">Part of a bilingual autoencoder (BAE) that shows the process for calculating the second part of the loss function as described below.</span>
      </div><!--tex4ht:label?: x1-3001r2 -->
      <!--l. 103-->
      <p class="noindent"></p>
    </div>
    <hr class="endfigure">
    <!--l. 106-->
    <p class="indent">Suppose we have a set of aligned $(x,y)$ sentence pairs in two languages, $X$ and $Y$ . <!--l. 109--></p>
    <p class="indent">We also assume vocabularies $V_X$ and $V_Y$ for each language, and $\mathbf{W}_X$ and $\mathbf{W}_Y$ of sizes $D$ <span class="cmsy-10">&#x00D7;</span> $V_X$ and $D$ <span class="cmsy-10">&#x00D7;</span> $V_Y$ respectively, as the word representation matrices as defined for a single language in section <a href="#x1-10001">1<!--tex4ht:ref: sec:bow --></a>. <!--l. 114--></p>
    <p class="indent">The encoder, similarly to before, produces:</p>
    <div class="eqnarray">
      <center class="math-display">
        $\phi\left(\mathbf{x}\right)	=	f\left(\mathbf{W}_{X}\mathbf{v}\left(\mathbf{x}\right)+\mathbf{b}\right)$
      </center>
      <center class="math-display">
        $\phi\left(\mathbf{y}\right)	=	f\left(\mathbf{W}_{Y}\mathbf{v}\left(\mathbf{y}\right)+\mathbf{b}\right)$
      </center>
    </div>where $\mathbf{b}$ is the same for both languages to keep the representations on roughly the same scale. Note that both have dimension $D$. <!--l. 122-->
    <p class="indent">The decoders are of the same form as before, but with individual parameters for each language. <!--l. 126--></p>
    <p class="noindent"><span class="paragraphHead"><a id="x1-40002"></a><span class="cmbx-10">The Loss Function</span></span> This encoder-decoder structure gives us the benefit of being able to calculate five different reconstruction errors:</p>
    <ol class="enumerate1">
      <li class="enumerate" id="x1-4002x1">Reconstruction of $\mathbf{y}$ given input $\mathbf{x}$.</li>
      <li class="enumerate" id="x1-4004x2">Reconstruction of $\mathbf{x}$ given input $\mathbf{y}$.</li>
      <li class="enumerate" id="x1-4006x3">Reconstruction of $\mathbf{x}$ given input $\mathbf{x}$.</li>
      <li class="enumerate" id="x1-4008x4">Reconstruction of $\mathbf{y}$ given input $\mathbf{y}$.</li>
      <li class="enumerate" id="x1-4010x5">Reconstruction of $\mathbf{x}$ and $\mathbf{y}$ given inputs $\mathbf{x}$ and $\mathbf{y}$.</li>
    </ol><!--l. 138-->
    <p class="noindent">For training, we can simply optimise a sum of these loss functions. <span class="cite">[<a href="#XRuder2012">3</a>,&#x00A0;<a href="#XLauly">4</a>,&#x00A0;<a href="#XChandar2014">5</a>]</span> <!--l. 142--></p>
    <p class="noindent"></p>
    <h3 class="sectionHead"><span class="titlemark">3</span> <a id="x1-50003"></a>Implementation</h3><!--l. 144-->
    <p class="noindent"></p>
    <hr class="figure">
    <div class="figure">
      <a id="x1-5001r3"></a> <!--l. 146-->
      <p class="noindent"><img class="smaller" alt="PIC" src="../img/nlp/bae-results.png"><br></p>
      <div class="caption">
        <span class="id">Figure&#x00A0;3:</span> <span class="content">Results achieved by Chandar Lauly et al using the methodology described above. The table shows six example words in English, along with the 10 closest words (by Euclidean distance) to each example, in both English and German. Source: <span class="cite">[<a href="#XChandar2014">5</a>]</span>.</span>
      </div><!--tex4ht:label?: x1-5001r3 -->
      <!--l. 153-->
      <p class="noindent"></p>
    </div>
    <hr class="endfigure">
    <!-- END: LyX Generated HTML -->
    <div class="navigation">
      <a class="button" href="word-phrase.html">Previous</a>
      <a class="button dark next" href="recursive.html">Next</a>
      <div class="clearfix"></div>
    </div>
  </div>
  <div class="well">
    <div class="container">
      <!-- BEGIN: LyX Generated HTML -->
      <h3 class="likesectionHead"><a id="x1-60003"></a>References</h3><!--l. 1-->
      <p class="noindent"></p>
      <div class="thebibliography">
        <p class="bibitem"><span class="biblabel">[1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XFergus"></a>Fergus R. Bag-of-Words models;. Available from: <a class="url" href="http://cs.nyu.edu/~fergus/teaching/vision_2012/9_BoW.pdf"><span class="cmtt-10">http://cs.nyu.edu/</span><span class="cmtt-10">~</span><span class="cmtt-10">fergus/teaching/vision_2012/9_BoW.pdf</span></a>.</p>
        <p class="bibitem"><span class="biblabel">[2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XNielsen2017"></a>Nielsen M. The cross-entropy cost function; 2017. Available from: <a class="url" href="http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function"><span class="cmtt-10">http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-_entropy_cost_function</span></a>.</p>
        <p class="bibitem"><span class="biblabel">[3]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XRuder2012"></a>Ruder S. A survey of cross-lingual embedding models; 2012. Available from: <a class="url" href="http://sebastianruder.com/cross-lingual-embeddings/"><span class="cmtt-10">http://sebastianruder.com/cross-_lingual-_embeddings/</span></a>.</p>
        <p class="bibitem"><span class="biblabel">[4]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XLauly"></a>Lauly S. CIFAR NCAP Summer School Presentation: An Autoencoder Approach to Learning Bilingual Word Representations;. Available from: <a class="url" href="http://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/StanislasLauly_presSummerSchoolCIFAR.pdf"><span class="cmtt-10">http://www.iro.umontreal.ca/</span><span class="cmtt-10">~</span><span class="cmtt-10">bengioy/cifar/NCAP2014-_summerschool/slides/StanislasLauly_presSummerSchoolCIFAR.pdf</span></a>.</p>
        <p class="bibitem"><span class="biblabel">[5]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XChandar2014"></a>Chandar APS, Lauly S, Larochelle H, Khapra MM, Ravindran B, Raykar VC, et&#x00A0;al. An Autoencoder Approach to Learning Bilingual Word Representations. CoRR. 2014;abs/1402.1454. Available from: <a class="url" href="http://arxiv.org/abs/1402.1454"><span class="cmtt-10">http://arxiv.org/abs/1402.1454</span></a>.</p>
      </div>
      <!-- END: LyX Generated HTML -->
    </div>
  </div>
  <script type="text/javascript" src="../js/jquery.min.js"></script>
  <script type="text/javascript" src="../js/global.js"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</body>
</html>
