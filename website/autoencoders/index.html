<!DOCTYPE html>
<html>
<head>
  <title>Autoencoders: Introduction</title>
  <link href="https://fonts.googleapis.com/css?family=Raleway:300|Source+Code+Pro" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="../css/normalise.css">
  <link rel="stylesheet" type="text/css" href="../css/global.css">
  <link rel="stylesheet" type="text/css" href="../css/content.css">
</head>
<body>
  <!-- Menu -->
  <span class="hamburger"></span>
  <nav class="hidden">
    <div class="brand">
      <a href="../"><img src="../img/logo.svg"> AUTOENCODERS</a>
    </div>
    <ul>
      <li><a href="#" content="neural-networks">Neural Networks</a></li>
      <li class="hidden" content="neural-networks">
        <ul>
          <li><a href="../neural-networks">Introduction</a></li>
          <li><a href="../neural-networks/neurons.html">Artificial Neurons</a></li>
          <li><a href="../neural-networks/learning.html">Learning</a></li>
          <li><a href="../neural-networks/regularisation.html">Regularisation</a></li>
          <li><a href="../neural-networks/optimisers.html">Optimisers</a></li>
        </ul>
      </li>
      <li><a href="#" content="autoencoders">Autoencoders</a></li>
      <li class="hidden" content="autoencoders">
        <ul>
          <li><a href="../autoencoders">Introduction</a></li>
          <li><a href="../autoencoders/denoising.html">Autoencoders for Denoising</a></li>
          <li><a href="../autoencoders/variational.html">Variational Autoencoders</a></li>
          <li><a href="../autoencoders/sparse.html">Sparse Autoencoders</a></li>
          <li><a href="../autoencoders/contractive.html">Contractive Autoencoders</a></li>
        </ul>
      </li>
      <li><a href="../autoencoders/ip.html">Autoencoders in Image Processing</a></li>
      <li class="last"><a href="#" content="nlp">Autoencoders in NLP</a></li>
      <li class="hidden last" content="nlp">
        <ul>
          <li><a href="../nlp">Introduction</a></li>
          <li><a href="../nlp/word-phrase.html">Word and Phrase Representations</a></li>
          <li><a href="../nlp/bilingual.html">Bilingual Phrase Representations</a></li>
          <li><a href="../nlp/recursive.html">Recursive Autoencoders</a></li>
          <li><a href="../nlp/sentiment.html">Sentiment Analysis</a></li>
          <li><a href="../nlp/paraphrase.html">Paraphrase Detection</a></li>
        </ul>
      </li>
    </ul>
    <div class="close"></div>
  </nav>
  <div class="overlay hidden"></div>

  <div class="container">
    <!-- BEGIN: LyX Generated HTML -->
    <h2 class="titleHead">Introduction to Autoencoders</h2>
    <h3 class="sectionHead"><span class="titlemark">1</span> <a id="x1-10001"></a>Introduction</h3><!--l. 32-->
    <p class="noindent">An autoencoder is a neural network that is trained to copy its input to its output, with the typical purpose of dimension reduction - the process of reducing the number of random variables under consideration. It features an encoder function to create a hidden layer (or multiple layers) which contains a code to describe the input. There is then a decoder which creates a reconstruction of the input from the hidden layer. An autoencoder can then become useful by having a hidden layer smaller than the input layer, forcing it to create a compressed representation of the data in the hidden layer by learning correlations in the data. This facilitates the classification, visualization, communication and storage of data <span class="cite">[<a href="#XG.E.Hinton2006">1</a>]</span>. Autoencoders are a form of unsupervised learning, meaning that an autoencoder only needs unlabeled data - a set of input data rather than input-output pairs. <!--l. 46--></p>
    <p class="noindent">Through an unsupervised learning algorithm, for linear reconstructions the autoencoder will try to learn a function $h_{W,b}(x)\approx x$, so as to minimize the mean square difference:</p>
    <center class="par-math-display">
      $L(x,y)=\sum(x-h_{W,b}(x))^{2}$
    </center><!--l. 52-->
    <p class="nopar"><!--l. 55--></p>
    <p class="noindent">where x is the input data and y is the reconstruction. However, when the decoder&#8217;s activation function is the Sigmoid function, the cross-entropy loss function is typically used:</p>
    <center class="par-math-display">
      $L(x,y)=-\sum_{i=1}^{d_{x}}x_{i}log(y_{i})+(1-x_{i})log(1-y_{i})$
    </center><!--l. 61-->
    <p class="nopar"><!--l. 64--></p>
    <p class="noindent">We can obtain optimum weights for this by starting with random weights and calculating a gradient. This is done by using the chain rule to <a href="../neural-networks/learning.html">back-propagate</a> error derivatives through the decoder network and then the encoder network. <!--l. 69--></p>
    <p class="noindent"></p>
    <div class="figure">
      <a id="x1-1001r1"></a> <!--l. 70-->
      <p class="noindent"><img alt="PIC" src="../img/autoencoders/autoencoder.png"><br></p>
      <div class="caption">
        <span class="id">Figure&#x00A0;1:</span> <span class="content">Layers in a autoencoder <span class="cite">[<a href="#Xautoencoder_visualization">2</a>]</span>.</span>
      </div><!--tex4ht:label?: x1-1001r1 -->
      <!--l. 75-->
      <p class="noindent"></p>
    </div>
    <h3 class="sectionHead"><span class="titlemark">2</span> <a id="x1-20002"></a>Uses</h3><!--l. 81-->
    <p class="noindent">The most intuitive application of autoencoders is data compression. Given an 256 x 256px image for example, a representation of a 28 x 28px may be learned, which is easier to handle. <!--l. 85--></p>
    <p class="noindent">We can also use <a href="denoising.html">denoising autoencoders</a> to reconstruct corrupted data, often in the form of images. <!--l. 88--></p>
    <p class="noindent">Another use is to <a href="denoising.html#pre-training">pre-train</a> deep networks with stacked denoising autoencoders. This is allows us to optimize deep learning solutions and avoid being stuck in local minima as we might be with random initialization of weights. <!--l. 94--></p>
    <p class="noindent"></p>
    <h3 class="sectionHead"><span class="titlemark">3</span> <a id="x1-30003"></a>History</h3><!--l. 96-->
    <p class="noindent">The idea of autoencoders was first mentioned in 1986, in an article extensively analyzing back-propagation <span class="cite">[<a href="#XRumelhrt1986">3</a>]</span>. In following years, the idea resurfaced in more research papers. A 1989 paper by Baldi and Hornik helped further introduce autoencoders by offering &#8220;a precise description of the salient features of the surface attached to E [the error function] when the units are linear&#8221; <span class="cite">[<a href="#XBaldi1989">4</a>]</span>. Another notable paper is by Hinton and Zemel from 1994 which describes a new objective function for training autoencoders that allows them to discover non-linear, factorial representations <span class="cite">[<a href="#XHinton1994">5</a>]</span>. However, it is hard to attribute the ideas about autoencoders because the literature is diverse and terminology has evolved over time. A currently emerging learning algorithm is the extreme learning machine, where the hidden node parameters are randomly generated and the output weights are computed thus learning a linear model, in a way faster than with back-propagation. It is worth noting how all currently used variants of autoencoders have been defined in only the last 10 years:</p>
    <ul class="itemize1">
      <li class="itemize">Vincent et al 2008 - <a href="../autoencoders/denoising.html">Denoising autoencoders</a></li>
      <li class="itemize">Goodfellow et al 2009 - <a href="../autoencoders/sparse.html">Sparse autoencoders</a></li>
      <li class="itemize">Rifai et al 2011 - <a href="../autoencoders/contractive.html">Contractive autoencoders</a></li>
      <li class="itemize">Kingma et al 2013 - <a href="../autoencoders/variational.html">Variational autoencoders</a></li>
    </ul><!--l. 1-->
    <p class="noindent"></p>
    <!-- END: LyX Generated HTML -->
    <div class="navigation">
      <a class="button dark next" href="denoising.html">Next</a>
      <div class="clearfix"></div>
    </div>
  </div>
  <div class="well">
    <div class="container">
      <!-- BEGIN: LyX Generated HTML -->
      <h3 class="likesectionHead"><a id="x1-40003"></a>References</h3><!--l. 1-->
      <p class="noindent"></p>
      <div class="thebibliography">
        <p class="bibitem"><span class="biblabel">[1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XG.E.Hinton2006"></a>Hinton GE, Salakhutdinov RR. Reducing the Dimensionality of Data with Neural Networks. In: Science; 2006. .</p>
        <p class="bibitem"><span class="biblabel">[2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="Xautoencoder_visualization"></a>et&#x00A0;al N. A dynamic programming approach to missing data estimation using neural networks;. Available from: <a class="url" href="https://www.researchgate.net/figure/222834127_fig1"><span class="cmtt-10">https://www.researchgate.net/figure/222834127_fig1</span></a>.</p>
        <p class="bibitem"><span class="biblabel">[3]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XRumelhrt1986"></a>Rumelhrt, Hinton, Williams. Learning internal representations by error propogation. Parallel distributed processing:; explorations in the microstructure of congition;Available from: <a class="url" href="http://dl.acm.org/citation.cfm?id=104293"><span class="cmtt-10">http://dl.acm.org/citation.cfm?id=104293</span></a>.</p>
        <p class="bibitem"><span class="biblabel">[4]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XBaldi1989"></a>Baldi, Hornik. Neural Networks and Principal Component Analysis: Learning from Examples Without Local Minima;. Available from: <a class="url" href="https://www.researchgate.net/publication/222438485"><span class="cmtt-10">https://www.researchgate.net/publication/222438485</span></a>.</p>
        <p class="bibitem"><span class="biblabel">[5]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XHinton1994"></a>Hinton, Zemel. Autoencoders, Minimum Description Length, and Helmholtz Free Energy. Advances in Neural Information Processsing Systems;Available from: <a class="url" href="https://www.cs.toronto.edu/~hinton/absps/cvq.pdf"><span class="cmtt-10">https://www.cs.toronto.edu/</span><span class="cmtt-10">~</span><span class="cmtt-10">hinton/absps/cvq.pdf</span></a>.</p>
      </div>
      <!-- END: LyX Generated HTML -->
    </div>
  </div>
  <script type="text/javascript" src="../js/jquery.min.js"></script>
  <script type="text/javascript" src="../js/global.js"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</body>
</html>
