<!DOCTYPE html>
<html>
<head>
  <title>Autoencoders: Sparse Autoencoders</title>
  <link href="https://fonts.googleapis.com/css?family=Raleway:300|Source+Code+Pro" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="../css/normalise.css">
  <link rel="stylesheet" type="text/css" href="../css/global.css">
  <link rel="stylesheet" type="text/css" href="../css/content.css">
</head>
<body>
  <!-- Menu -->
  <span class="hamburger"></span>
  <nav class="hidden">
    <div class="brand">
      <a href="../"><img src="../img/logo.svg"> AUTOENCODERS</a>
    </div>
    <ul>
      <li><a href="#" content="neural-networks">Neural Networks</a></li>
      <li class="hidden" content="neural-networks">
        <ul>
          <li><a href="../neural-networks">Introduction</a></li>
          <li><a href="../neural-networks/neurons.html">Artificial Neurons</a></li>
          <li><a href="../neural-networks/learning.html">Learning</a></li>
          <li><a href="../neural-networks/regularisation.html">Regularisation</a></li>
          <li><a href="../neural-networks/optimisers.html">Optimisers</a></li>
        </ul>
      </li>
      <li><a href="#" content="autoencoders">Autoencoders</a></li>
      <li class="hidden" content="autoencoders">
        <ul>
          <li><a href="../autoencoders">Introduction</a></li>
          <li><a href="../autoencoders/denoising.html">Autoencoders for Denoising</a></li>
          <li><a href="../autoencoders/variational.html">Variational Autoencoders</a></li>
          <li><a href="../autoencoders/sparse.html">Sparse Autoencoders</a></li>
          <li><a href="../autoencoders/contractive.html">Contractive Autoencoders</a></li>
        </ul>
      </li>
      <li><a href="../autoencoders/ip.html">Autoencoders in Image Processing</a></li>
      <li class="last"><a href="#" content="nlp">Autoencoders in NLP</a></li>
      <li class="hidden last" content="nlp">
        <ul>
          <li><a href="../nlp">Introduction</a></li>
          <li><a href="../nlp/word-phrase.html">Word and Phrase Representations</a></li>
          <li><a href="../nlp/bilingual.html">Bilingual Phrase Representations</a></li>
          <li><a href="../nlp/recursive.html">Recursive Autoencoders</a></li>
          <li><a href="../nlp/sentiment.html">Sentiment Analysis</a></li>
          <li><a href="../nlp/paraphrase.html">Paraphrase Detection</a></li>
        </ul>
      </li>
    </ul>
    <div class="close"></div>
  </nav>
  <div class="overlay hidden"></div>

  <div class="container">
    <!-- BEGIN: LyX Generated HTML -->
    <h2 class="titleHead">Sparse Autoencoders</h2>
    While autoencoders normally discover useful structures by having a small number of hidden units, they can also be useful with a large number of hidden units. By doing so, the autoencoder enlarges the given input&#8217;s representation. This is possible by introducing a sparsity constraint. The aim of this, is to cause the large number of neurons to all have a low average output so that the neurons are inactive most of the time. If we are using a Sigmoid activation function we would want their output to be as close to 0 as possible, and as close to -1 as possible if using a Tanh activation function. <!--l. 36-->
    <p class="noindent">If we have the activation function of a neuron, $a_{j}$, we can calculate the average activation function of all the neurons, $p_{j}$, with the formula:</p>
    <center class="par-math-display">
      $p_{j}=\frac{1}{m}\sum_{i=1}^{m}\left[a_{j}x\right]$
    </center><!--l. 42-->
    <p class="nopar"><!--l. 45--></p>
    <p class="noindent">The aim of the sparsity constraint is to minimize $p_{j}$, so that $p_{j}=p_{c}$ where $p_{c}$ is a small number close to 0 (for the Sigmoid activation function), such as 0.05. We can do so by adding a penalty term to mean squared error cost function that we normally try to minimize for classical autoencoders. The penalty term, as with variational autoencoders, is the KL divergence between the Bernoulli random variables $p_{j}$ and $p_{c}$, and can be calculated with the formula:</p>
    <center class="par-math-display">
      $KL(p_{j}||p_{c})=plog\frac{p_{c}}{p_{j}}+(1-p)log\frac{1-p_{c}}{1-p_{j}}$
    </center><!--l. 56-->
    <p class="nopar"><!--l. 59--></p>
    <p class="noindent"></p>
    <div class="figure">
      <a id="x1-2r1"></a> <!--l. 61-->
      <p class="noindent"><img alt="PIC" src="../img/autoencoders/kl-divergence.png"><br></p>
      <div class="caption">
        <span class="id">Figure&#x00A0;1:</span> <span class="content">KL divergence penalty term for $p_{c}=0.2$ <span class="cite">[<a href="#XCS294A_Lecture_notes">1</a>]</span>.</span>
      </div><!--tex4ht:label?: x1-2r1 -->
      <!--l. 65-->
      <p class="noindent"></p>
    </div>
    <!--l. 68-->
    <p class="noindent">An advancement to sparse autoencoders is the k-sparse autoencoder. This is where we choose k neurons with the highest activation functions and ignore the others, by either sorting the activities or using ReLU activation functions and adaptively adjusting the thresholds until the k largest neurons are identified. This lets us tune the value of k to obtain a sparsity level most suitable for our dataset. A large sparsity level would learn very local features which may not be useful for identifying handwritten digits but useful for pretraining neural nets <span class="cite">[<a href="#XAlirezaMakhzani2013">2</a>]</span>. <!--l. 78--></p>
    <p class="noindent"></p>
    <div class="figure">
      <a id="x1-3r2"></a> <!--l. 79-->
      <p class="noindent"><img alt="PIC" src="../img/autoencoders/k-sparse-autoencoder.png"><br></p>
      <div class="caption">
        <span class="id">Figure&#x00A0;2:</span> <span class="content">Different sparsity levels k, learned from MNIST with 1000 hidden units <span class="cite">[<a href="#XAlirezaMakhzani2013">2</a>]</span>.</span>
      </div><!--tex4ht:label?: x1-3r2 -->
      <!--l. 81-->
      <p class="noindent"></p>
    </div>
    <!-- END: LyX Generated HTML -->
    <div class="navigation">
      <a class="button" href="variational.html">Previous</a>
      <a class="button dark next" href="contractive.html">Next</a>
      <div class="clearfix"></div>
    </div>
  </div>
  <div class="well">
    <div class="container">
      <!-- BEGIN: LyX Generated HTML -->
      <h3 class="likesectionHead"><a id="x1-1000"></a>References</h3><!--l. 1-->
      <p class="noindent"></p>
      <div class="thebibliography">
        <p class="bibitem"><span class="biblabel">[1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XCS294A_Lecture_notes"></a>NG A. Sparse autoencoder. CS294A Lecture notes;.</p>
        <p class="bibitem"><span class="biblabel">[2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a id="XAlirezaMakhzani2013"></a>Alireza&#x00A0;Makhzani BF. k-Sparse Autoencoders. arXiv. 2013;.</p>
      </div>
      <!-- END: LyX Generated HTML -->
      <div class="navigation">
        <a class="button" href="variational.html">Previous</a>
        <a class="button dark next" href="contractive.html">Next</a>
        <div class="clearfix"></div>
      </div>
    </div>
  </div>
  <script type="text/javascript" src="../js/jquery.min.js"></script>
  <script type="text/javascript" src="../js/global.js"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</body>
</html>
