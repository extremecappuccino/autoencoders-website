<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Variational Autoencoders</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html --> 
<meta name="src" content="vae.tex"> 
<meta name="date" content="2017-03-16 01:41:00"> 
<link rel="stylesheet" type="text/css" href="vae.css"> 
</head><body 
>
<div class="maketitle">
                                                                                         
                                                                                         
                                                                                         
                                                                                         

<h2 class="titleHead">Variational Autoencoders</h2>
<div class="author" ><span 
class="cmr-12">Nikolai Smirnov</span></div>
<br />
<div class="date" ><span 
class="cmr-12">March 16, 2017</span></div>
</div>A variational autoencoder (VAE) resembles a classical autoencoder and is a neural network consisting of an encoder,
a decoder and a loss function. They let us design generative models of data and fit them to large data-sets, and can
also be used for image generation and reinforcement learning. For example, a practical application may be to
generate trees for a forest in a video game, which are all similar but not the same. They are a recent advancement in
machine learning, having only been defined in 2013. However, they solve a long standing problem in
machine-learning and do so with weak assumptions and fast training via back-propagation, which explains their fast
rise in popularity <span class="cite">[<a 
href="#XDoersch2016">1</a>]</span>.
<!--l. 41--><p class="noindent" >The first layer of the VAE is the encoder which will take the input and convert it into a latent vector. This could be
done by reducing the mean squared error of the input and output, like a standard autoencoder. With images for
example, we can now represent something like a picture of a cat as the vector [1.9, 8.2, 2.1]. The vector is called
latent because given just an output from the model, we don&#8217;t necessarily know which settings of the
variables in the latent vector generated this output, without inferring it using something like computer
vision.
<!--l. 50--><p class="noindent" >However to make the VAE a generative model, we must add a constraint on the encoding network that forces it to
generate latent vectors that roughly follow a Gaussian distribution. This is the key feature of variational
autoencoders, and allows the user to generate an output similar to the database the VAE was trained on by
inputting a latent vector straight to the decoder. The problem now, is to make the network&#8217;s latent variables match
the unit Gaussian distribution as closely as possible while also accurately providing an output similar to the
input.
<!--l. 60--><p class="noindent" >For a mathematically simplified explanation, this is done by changing our loss term to be the sum of the mean
squared error and a latent loss. The mean squared error as usual, allows us to measure how accurately the network
reconstructs images. The latent loss however, is the Kullback-Liebler divergence (KL divergence), which can measure
how closely the variables match a unit Gaussian distribution. The encoder can now be changed to generate a vector
of means and a vector of standard deviations, rather than a vector of real variables. From this the KL divergence
can be calculated.
<!--l. 70--><p class="noindent" ><hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
<a 
 id="x1-2r1"></a>
                                                                                         
                                                                                         

<!--l. 72--><p class="noindent" ><img 
src="0_home_jordanspooner_Documents_163_Computing_To___ncoders-website_about_2-vae_res_vae-diagram.jpg" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;1: </span><span  
class="content">Simplification of layers in a variational autoencoder <span class="cite">[<a 
href="#Xkvfrans">2</a>]</span>.</span></div><!--tex4ht:label?: x1-2r1 -->
                                                                                         
                                                                                         
<!--l. 76--><p class="noindent" ></div><hr class="endfigure">
<h3 class="likesectionHead"><a 
 id="x1-1000"></a>References</h3>
<!--l. 1--><p class="noindent" >
   <div class="thebibliography">
   <p class="bibitem" ><span class="biblabel">
 [1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XDoersch2016"></a>Doersch      C.      Tutorial      on      Variational      Autoencoders;.                Available      from:
   <a 
href="https://arxiv.org/abs/1606.05908" class="url" ><span 
class="cmtt-10">https://arxiv.org/abs/1606.05908</span></a>.
   </p>
   <p class="bibitem" ><span class="biblabel">
 [2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xkvfrans"></a>Frans       K.       Variational       Autoencoders       Explained;.                   Available       from:
   <a 
href="http://kvfrans.com/variational-autoencoders-explained/" class="url" ><span 
class="cmtt-10">http://kvfrans.com/variational-_autoencoders-_explained/</span></a>.
</p>
   </div>
 
</body></html> 

                                                                                         


