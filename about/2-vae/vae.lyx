#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[hidelinks]{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Variational Autoencoders
\end_layout

\begin_layout Author
Nikolai Smirnov
\end_layout

\begin_layout Standard
A variational autoencoder (VAE) resembles a classical autoencoder and is
 a neural network consisting of an encoder, a decoder and a loss function.
 They let us design generative models of data and fit them to large data-sets,
 and can also be used for image generation and reinforcement learning.
 For example, a practical application may be to generate trees for a forest
 in a video game, which are all similar but not the same.
 They are a recent advancement in machine learning, having only been defined
 in 2013.
 However, they solve a long standing problem in machine-learning and do
 so with weak assumptions and fast training via back-propagation, which
 explains their fast rise in popularity 
\begin_inset CommandInset citation
LatexCommand cite
key "Doersch2016"

\end_inset

.
\end_layout

\begin_layout Standard
The first layer of the VAE is the encoder which will take the input and
 convert it into a latent vector.
 This could be done by reducing the mean squared error of the input and
 output, like a standard autoencoder.
 With images for example, we can now represent something like a picture
 of a cat as the vector [1.9, 8.2, 2.1].
 The vector is called latent because given just an output from the model,
 we don't necessarily know which settings of the variables in the latent
 vector generated this output, without inferring it using something like
 computer vision.
\end_layout

\begin_layout Standard
However to make the VAE a generative model, we must add a constraint on
 the encoding network that forces it to generate latent vectors that roughly
 follow a Gaussian distribution.
 This is the key feature of variational autoencoders, and allows the user
 to generate an output similar to the database the VAE was trained on by
 inputting a latent vector straight to the decoder.
 The problem now, is to make the network's latent variables match the unit
 Gaussian distribution as closely as possible while also accurately providing
 an output similar to the input.
 
\end_layout

\begin_layout Standard
For a mathematically simplified explanation, this is done by changing our
 loss term to be the sum of the mean squared error and a latent loss.
 The mean squared error as usual, allows us to measure how accurately the
 network reconstructs images.
 The latent loss however, is the Kullback-Liebler divergence (KL divergence),
 which can measure how closely the variables match a unit Gaussian distribution.
 The encoder can now be changed to generate a vector of means and a vector
 of standard deviations, rather than a vector of real variables.
 From this the KL divergence can be calculated.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename res/vae-diagram.jpg
	lyxscale 70
	width 80text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Simplification of layers in a variational autoencoder 
\begin_inset CommandInset citation
LatexCommand cite
key "kvfrans"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "vae"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
