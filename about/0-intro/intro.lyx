#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[hidelinks]{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "" "default"
\font_sans "" "default"
\font_typewriter "" "default"
\font_math "" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Introduction to Autoencoders
\end_layout

\begin_layout Author
Nikolai Smirnov
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
An autoencoder is a neural network that is trained to copy its input to
 its output, with the typical purpose of dimension reduction - the process
 of reducing the number of random variables under consideration.
 It features an encoder function to create a hidden layer (or multiple layers)
 which contains a code to describe the input.
 There is then a decoder which creates a reconstruction of the input from
 the hidden layer.
 An autoencoder can then become useful by having a hidden layer smaller
 than the input layer, forcing it to create a compressed representation
 of the data in the hidden layer by learning correlations in the data.
 This facilitates the classification, visualization, communication and storage
 of data 
\begin_inset CommandInset citation
LatexCommand cite
key "G.E.Hinton2006"

\end_inset

.
 Autoencoders are a form of unsupervised learning, meaning that an autoencoder
 only needs unlabeled data - a set of input data rather than input-output
 pairs.
\end_layout

\begin_layout Standard
Through an unsupervised learning algorithm, for linear reconstructions the
 autoencoder will try to learn a function 
\begin_inset Formula $h_{W,b}(x)\approx x$
\end_inset

, so as to minimize the mean square difference: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(x,y)=\sum(x-h_{W,b}(x))^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where x is the input data and y is the reconstruction.
 However, when the decoder's activation function is the Sigmoid function,
 the cross-entropy loss function is typically used:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(x,y)=-\sum_{i=1}^{d_{x}}x_{i}log(y_{i})+(1-x_{i})log(1-y_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
We can obtain optimum weights for this by starting with random weights and
 calculating a gradient.
 This is done by using the chain rule to ***back-propagate*** error derivatives
 through the decoder network and then the encoder network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename res/autoencoder.png
	width 50text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Layers in a autoencoder 
\begin_inset CommandInset citation
LatexCommand cite
key "autoencoder_visualization"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Uses
\end_layout

\begin_layout Standard
The most intuitive application of autoencoders is ***data compression***.
 Given an 256 x 256px image for example, a representation of a 28 x 28px
 may be learned, which is easier to handle.
\end_layout

\begin_layout Standard
We can also use denoising ***autoencoders*** to reconstruct corrupted data,
 often in the form of images.
\end_layout

\begin_layout Standard
Another use is to ***pre-train*** ***deep networks*** with stacked denoising
 autoencoders.
 This is allows us to optimize deep learning solutions and avoid being stuck
 in local minima as we might be with random initialization of weights.
\end_layout

\begin_layout Section
History
\end_layout

\begin_layout Standard
The idea of autoencoders was first mentioned in 1986, in an article extensively
 analyzing back-propagation 
\begin_inset CommandInset citation
LatexCommand cite
key "Rumelhrt1986"

\end_inset

.
 In following years, the idea resurfaced in more research papers.
 A 1989 paper by Baldi and Hornik helped further introduce autoencoders
 by offering 
\begin_inset Quotes eld
\end_inset

a precise description of the salient features of the surface attached to
 E [the error function] when the units are linear
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Baldi1989"

\end_inset

.
 Another notable paper is by Hinton and Zemel from 1994 which describes
 a new objective function for training autoencoders that allows them to
 discover non-linear, factorial representations 
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton1994"

\end_inset

.
 However, it is hard to attribute the ideas about autoencoders because the
 literature is diverse and terminology has evolved over time.
 A currently emerging learning algorithm is the extreme learning machine,
 where the hidden node parameters are randomly generated and the output
 weights are computed thus learning a linear model, in a way faster than
 with back-propagation.
 It is worth noting how all currently used variants of autoencoders have
 been defined in only the last 10 years:
\end_layout

\begin_layout Itemize
Vincent et al 2008 - ***Denoising autoencoders***
\end_layout

\begin_layout Itemize
Goodfellow et al 2009 - ***Sparse autoencoders***
\end_layout

\begin_layout Itemize
Rifai et al 2011 - ***Contractive autoencoders***
\end_layout

\begin_layout Itemize
Kingma et al 2013 - ***Variational autoencoders***
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "intro"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
