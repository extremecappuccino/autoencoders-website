#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Introduction to Autoencoders
\end_layout

\begin_layout Author
Nikolai Smirnov
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard

\series bold
1.0 Intro
\end_layout

\begin_layout Standard
An autoencoder is a neural network that is trained to copy its input to
 its output, with the typical purpose of dimenson reduction - the process
 of reducing the number of random variables under consideration.
 It features an encoder function to create a smaller hidden layer (or multiple
 layers) which contains a code to describe the input.
 There is then a decoder which creates a reconstruciton of the input from
 the hidden layer.
 An autoencoder can then become useful by only approximately copying the
 input to the output, when restricted to only learn useful properties of
 the data.
 This facilitates the classsification, visulization, communication and storage
 of data.
\begin_inset CommandInset citation
LatexCommand cite
key "G.E.Hinton2006"

\end_inset


\end_layout

\begin_layout Standard
Through an unsupervised learning algorithm, the autoencoder will try to
 learn a fuction
\begin_inset Formula $h_{W,b}(x)\approx x$
\end_inset

, so as to minimise the mean square difference 
\begin_inset Formula $\sum(x-h_{W,b}(x))^{2}$
\end_inset

.
 We can obtain optimum weights for this by starting wtih random weights
 and calculating a gradient.
 This is done by using the chain rule to backpropogate [LINK TO QIANG] error
 derivatives through the decoder network and then the encoder network.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /homes/ns4516/Downloads/Screenshot-from-2014-05-09-233355.png

\end_inset


\end_layout

\begin_layout Standard
https://blog.keras.io/building-autoencoders-in-keras.html
\end_layout

\begin_layout Standard

\series bold
1.1 Uses
\end_layout

\begin_layout Standard
The most intuative application of autoencoders is data compression [LINK
 TO DATA COMPRESSION].
 Given an 256x256px image for example, a representation of a 28x28px may
 be learnt, which is easier to handle.
 We can also use denoising autoencoders [LINK TO DENOISING] to reconstruct
 corrupted data, often in the form of images.
 
\end_layout

\begin_layout Standard
Another use is to pre-train deep networks [LINK TO DEEP NETWORKS].
 By using a stacked autoencoder [LINK TO STACKED AUTOENCODERS] in an unsupervise
d manner, we can learn starting weights prior to the supervised fine-tuning
 training procedure with deep learning.
 A regularization effect is then created as a result of the pre-training
 procedure creating an initialization point, to which the parameters are
 restricted to a small volume of parameter space that is delineated by the
 boundry of the local basin of attraction [LINK TO SOMETHING THAT LETS THIS
 MAKE SENSE].
 
\begin_inset CommandInset citation
LatexCommand cite
key "ERHAN2010"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename Pretraining Deep Networks.png

\end_inset


\end_layout

\begin_layout Standard
The above image, created by 
\begin_inset CommandInset citation
LatexCommand cite
key "ERHAN2010"

\end_inset

 is a 2D visulaization generated with ISOMAP of the functions represented
 by 50 networks with and 50 networks without pre-traning, for supervised
 training over MNIST (handwritten digits database) with 2 hidden layers.
 The colours from deep blue to cyan represent the progression in iterations.
 As the image shows, the pre-training models all start in a small region
 of space and decrease in variance, towards a basin of attraction [SOMEONE
 CONFIRM PLEASE].
 The traning was also quicker by pre-training the functions.
\end_layout

\begin_layout Standard
Autoencoders can also be used to solve the one-class classifcation problem
 with raw data.
 This is where a neural network must be trained to identify an object from
 a specific class, but learning from a tranining set containing only objects
 of that class, for example to test if a nuclear power plant is peforming
 normally, since there have been few power plant accidents.
 Unlike other deep-learning architectures, autoencoders can be trained on
 one type of data and create a threshold, based on the maximum reconstruction
 error in the training set, to determine if a sample is a member of the
 class.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ShehrozS.Khan2014"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "intro"
options "plain"

\end_inset


\end_layout

\begin_layout Standard

\series bold
1.2 History
\end_layout

\begin_layout Standard
The idea of autoencoders was first mentioned in 1986, in an article extensively
 analysing backpropogation 
\begin_inset CommandInset citation
LatexCommand cite
key "Rumelhrt1986"

\end_inset

.
 In following years, the idea resurfaced in more research papers.
 A 1988 paper by Boulard and Kamp on auto-association showed that 
\begin_inset Quotes eld
\end_inset

for auto-association with linear output units, the optimal weight values
 can be derived by standard linear algebra ...
 making thus the nonlinear functions at the hidden layer completely unnecessary
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Bourlard1988"

\end_inset

.
 Another notable paper is by Hinton and Zemel from 1994 which describes
 a new objective function for training autoencoders that allows them to
 discover non-linear, factorial representations
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton1994"

\end_inset

.
 However, it is hard to attribute the ideas about autoencoders because the
 literature is diverse and terminology has evolved over time.
 A currently emerging learning algorithm is the extreme learning machine,
 where the hidden node parameters are randomly generated and the output
 weights are computed thus learning a linear model, in a way faster than
 with backpropogation.
\end_layout

\begin_layout Section

\series bold
Structure
\end_layout

\begin_layout Section

\series bold
Variations
\end_layout

\begin_layout Section
References
\end_layout

\begin_layout Standard
http://science.sciencemag.org/content/313/5786/504.full
\end_layout

\begin_layout Standard
http://www.deeplearningbook.org/contents/autoencoders.html
\end_layout

\begin_layout Standard
http://yann.lecun.com/ex/research/
\end_layout

\begin_layout Standard
http://ace.cs.ohiou.edu/~razvan/courses/dl6890/papers/bourlard-kamp88.pdf
\end_layout

\begin_layout Standard
https://www.cs.toronto.edu/~hinton/absps/cvq.pdf
\end_layout

\begin_layout Standard
http://ai.stanford.edu/~quocle/tutorial2.pdf - autoencoders vs data compression
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "intro"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
