#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Introduction to Autoencoders
\end_layout

\begin_layout Author
Nikolai Smirnov
\end_layout

\begin_layout Standard

\series bold
1 Introduction
\end_layout

\begin_layout Standard
An autoencoder is a neural network that is trained to copy its input to
 its output, with the typical purpose of dimenson reduction.
 It features an encoder function to create a hidden layer which contains
 a code to describe the input.
 There is also then a decoder which creates a reconstruciton of the input
 from the hidden layer.
 An autoencoder can then become useful by only approximately copying the
 input to the output, when restricted to only learn useful properties of
 the data.
 
\end_layout

\begin_layout Standard
Through an unsupervised learning algorithm, the autoencoder will try to
 learn a fuction 
\begin_inset Formula $h_{W,b}(x)\approx x$
\end_inset

, so as to minimise the mean square difference 
\begin_inset Formula $\sum(x-h_{W,b}(x))^{2}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
2 History
\end_layout

\begin_layout Standard
LeCun 1987, Boulard and Kamp 1987, Zemel and Hinton 1994
\end_layout

\begin_layout Standard

\series bold
3 Structure
\end_layout

\begin_layout Standard

\series bold
4 Variations
\end_layout

\begin_layout Standard

\series bold
References
\end_layout

\begin_layout Standard
http://science.sciencemag.org/content/313/5786/504.full
\end_layout

\begin_layout Standard
http://www.deeplearningbook.org/contents/autoencoders.html
\end_layout

\begin_layout Standard
http://yann.lecun.com/ex/research/
\end_layout

\begin_layout Standard
http://ace.cs.ohiou.edu/~razvan/courses/dl6890/papers/bourlard-kamp88.pdf
\end_layout

\begin_layout Standard
https://www.cs.toronto.edu/~hinton/absps/cvq.pdf
\end_layout

\begin_layout Standard
http://ai.stanford.edu/~quocle/tutorial2.pdf - autoencoders vs data compression
\end_layout

\end_body
\end_document
