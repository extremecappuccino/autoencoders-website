#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "" "default"
\font_sans "" "default"
\font_typewriter "" "default"
\font_math "" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard

\series bold
1 Introduction
\end_layout

\begin_layout Standard

\series bold
1.0 Intro
\end_layout

\begin_layout Standard
An autoencoder is a neural network that is trained to copy its input to
 its output, with the typical purpose of dimension reduction - the process
 of reducing the number of random variables under consideration.
 It features an encoder function to create a hidden layer (or multiple layers)
 which contains a code to describe the input.
 There is then a decoder which creates a reconstruction of the input from
 the hidden layer.
 An autoencoder can then become useful by having a hidden layer smaller
 than the input layer, forcing it to create a compressed representation
 of the data in the hidden layer by learning correlations in the data.
 This facilitates the classification, visualization, communication and storage
 of data.
\begin_inset CommandInset citation
LatexCommand cite
key "G.E.Hinton2006"

\end_inset

.
 Autoencoders are a form of unsupervised learning, meaning that an autoencoder
 only needs unlabeled data - a set of input data rather than input-output
 pairs.
\end_layout

\begin_layout Standard
Through an unsupervised learning algorithm, for linear reconstructions the
 autoencoder will try to learn a function
\begin_inset Formula $h_{W,b}(x)\approx x$
\end_inset

, so as to minimize the mean square difference: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(x,y)=\sum(x-h_{W,b}(x))^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where x is the input data and y is the reconstruction.
 However, when the decoder's activation function is the sigmoid function,
 the cross-entropy loss function is typically used:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
L(x,y)=-\sum_{i=1}^{d_{x}}x_{i}log(y_{i})+(1-x_{i})log(1-y_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
We can obtain optimum weights for this by starting with random weights and
 calculating a gradient.
 This is done by using the chain rule to backpropogate [LINK TO QIANG] error
 derivatives through the decoder network and then the encoder network.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename autoencoder.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Layers in a autoencoder
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
https://blog.keras.io/building-autoencoders-in-keras.html
\end_layout

\begin_layout Standard

\series bold
1.1 Uses
\end_layout

\begin_layout Standard
The most intuitive application of autoencoders is data compression [LINK
 TO DATA COMPRESSION].
 Given an 256x256px image for example, a representation of a 28x28px may
 be learnt, which is easier to handle.
\end_layout

\begin_layout Standard
We can also use denoising autoencoders [LINK TO DENOISING] to reconstruct
 corrupted data, often in the form of images.
 
\end_layout

\begin_layout Standard
Another use is to pre-train deep networks [LINK TO DEEP NETWORKS] [LINK
 TO PRE-TRAINING] with stacked denoising autoencoders.
 This is allows us to optimize deep learning solutions and avoid being stuck
 in local minima as we might be with random initialization of weights.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "intro"
options "plain"

\end_inset


\end_layout

\begin_layout Standard

\series bold
1.2 History
\end_layout

\begin_layout Standard
The idea of autoencoders was first mentioned in 1986, in an article extensively
 analyzing backpropogation 
\begin_inset CommandInset citation
LatexCommand cite
key "Rumelhrt1986"

\end_inset

.
 In following years, the idea resurfaced in more research papers.
 A 1989 paper by Baldi and Hornik helped further introduce autoencoders
 by offering 
\begin_inset Quotes eld
\end_inset

a precise description of the salient features of the surface attached to
 E [the error function] when the units are linear
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Baldi1989"

\end_inset

.
 Another notable paper is by Hinton and Zemel from 1994 which describes
 a new objective function for training autoencoders that allows them to
 discover non-linear, factorial representations
\begin_inset CommandInset citation
LatexCommand cite
key "Hinton1994"

\end_inset

.
 However, it is hard to attribute the ideas about autoencoders because the
 literature is diverse and terminology has evolved over time.
 A currently emerging learning algorithm is the extreme learning machine,
 where the hidden node parameters are randomly generated and the output
 weights are computed thus learning a linear model, in a way faster than
 with backpropogation.
 It is worth noting how all currently used variants of autoencoders have
 been defined in only the last 10 years:
\end_layout

\begin_layout Itemize
Vincent et al 2008 - Denoising autoencoders [link]
\end_layout

\begin_layout Itemize
Goodfellow et al 2009 - Sparse autoencoders [link]
\end_layout

\begin_layout Itemize
Rifai et al 2011 - Contractive autoencoders [link]
\end_layout

\begin_layout Itemize
Kingma et al 2013 - Variational autoencoders [link]
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "intro"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
