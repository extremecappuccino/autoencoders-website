<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Introduction to Autoencoders</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html --> 
<meta name="src" content="intro.tex"> 
<meta name="date" content="2017-03-16 01:38:00"> 
<link rel="stylesheet" type="text/css" href="intro.css"> 
</head><body 
>
<div class="maketitle">
                                                                                         
                                                                                         
                                                                                         
                                                                                         

<h2 class="titleHead">Introduction to Autoencoders</h2>
<div class="author" ><span 
class="cmr-12">Nikolai Smirnov</span></div>
<br />
<div class="date" ><span 
class="cmr-12">March 16, 2017</span></div>
</div>
<h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-10001"></a>Introduction</h3>
<!--l. 32--><p class="noindent" >An autoencoder is a neural network that is trained to copy its input to its output, with the typical purpose of
dimension reduction - the process of reducing the number of random variables under consideration. It features an
encoder function to create a hidden layer (or multiple layers) which contains a code to describe the input. There is
then a decoder which creates a reconstruction of the input from the hidden layer. An autoencoder can then become
useful by having a hidden layer smaller than the input layer, forcing it to create a compressed representation of
the data in the hidden layer by learning correlations in the data. This facilitates the classification,
visualization, communication and storage of data <span class="cite">[<a 
href="#XG.E.Hinton2006">1</a>]</span>. Autoencoders are a form of unsupervised learning,
meaning that an autoencoder only needs unlabeled data - a set of input data rather than input-output
pairs.
<!--l. 46--><p class="noindent" >Through an unsupervised learning algorithm, for linear reconstructions the autoencoder will try to learn a function
<span 
class="cmmi-10">h</span><sub><span 
class="cmmi-7">W,b</span></sub>(<span 
class="cmmi-10">x</span>) <span 
class="cmsy-10">&asymp; </span><span 
class="cmmi-10">x</span>, so as to minimize the mean square difference:
<center class="par-math-display" >
<img 
src="intro0x.png" alt="        &sum;
L(x,y) =   (x&minus; hW,b(x))2
" class="par-math-display" ></center>
<!--l. 52--><p class="nopar" >
<!--l. 55--><p class="noindent" >where x is the input data and y is the reconstruction. However, when the decoder&#8217;s activation function is the
Sigmoid function, the cross-entropy loss function is typically used:
<center class="par-math-display" >
<img 
src="intro1x.png" alt="           dx
L (x,y) = &minus; &sum;  xlog(y)+ (1&minus; x )log(1&minus; y )
           i=1  i    i       i        i
" class="par-math-display" ></center>
<!--l. 61--><p class="nopar" >
                                                                                         
                                                                                         
<!--l. 64--><p class="noindent" >We can obtain optimum weights for this by starting with random weights and calculating a gradient. This is done by
using the chain rule to ***back-propagate*** error derivatives through the decoder network and then the encoder
network.
<!--l. 69--><p class="noindent" ><hr class="figure"><div class="figure" 
>
                                                                                         
                                                                                         
<a 
 id="x1-1001r1"></a>
                                                                                         
                                                                                         
<!--l. 70--><p class="noindent" > <img 
src="0_home_jordanspooner_Documents_163_Computing_To___oders-website_about_0-intro_res_autoencoder.png" alt="PIC"  
>
<br /> <div class="caption" 
><span class="id">Figure&#x00A0;1: </span><span  
class="content">Layers in a autoencoder <span class="cite">[<a 
href="#Xautoencoder_visualization">2</a>]</span>.</span></div><!--tex4ht:label?: x1-1001r1 -->
                                                                                         
                                                                                         
<!--l. 75--><p class="noindent" ></div><hr class="endfigure">
<h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-20002"></a>Uses</h3>
<!--l. 81--><p class="noindent" >The most intuitive application of autoencoders is ***data compression***. Given an 256 x 256px image for example,
a representation of a 28 x 28px may be learned, which is easier to handle.
<!--l. 85--><p class="noindent" >We can also use denoising ***autoencoders*** to reconstruct corrupted data, often in the form of
images.
<!--l. 88--><p class="noindent" >Another use is to ***pre-train*** ***deep networks*** with stacked denoising autoencoders. This is allows us to
optimize deep learning solutions and avoid being stuck in local minima as we might be with random initialization of
weights.
<!--l. 94--><p class="noindent" >
<h3 class="sectionHead"><span class="titlemark">3   </span> <a 
 id="x1-30003"></a>History</h3>
<!--l. 96--><p class="noindent" >The idea of autoencoders was first mentioned in 1986, in an article extensively analyzing back-propagation <span class="cite">[<a 
href="#XRumelhrt1986">3</a>]</span>. In
following years, the idea resurfaced in more research papers. A 1989 paper by Baldi and Hornik helped further
introduce autoencoders by offering &#8220;a precise description of the salient features of the surface attached to E [the
error function] when the units are linear&#8221; <span class="cite">[<a 
href="#XBaldi1989">4</a>]</span>. Another notable paper is by Hinton and Zemel from 1994 which
describes a new objective function for training autoencoders that allows them to discover non-linear,
factorial representations <span class="cite">[<a 
href="#XHinton1994">5</a>]</span>. However, it is hard to attribute the ideas about autoencoders because the
literature is diverse and terminology has evolved over time. A currently emerging learning algorithm is the
extreme learning machine, where the hidden node parameters are randomly generated and the output
weights are computed thus learning a linear model, in a way faster than with back-propagation. It is
worth noting how all currently used variants of autoencoders have been defined in only the last 10
years:
     <ul class="itemize1">
     <li class="itemize">Vincent et al 2008 - ***Denoising autoencoders***
     </li>
     <li class="itemize">Goodfellow et al 2009 - ***Sparse autoencoders***
     </li>
     <li class="itemize">Rifai et al 2011 - ***Contractive autoencoders***
     </li>
     <li class="itemize">Kingma et al 2013 - ***Variational autoencoders***</li></ul>
<!--l. 1--><p class="noindent" >
<h3 class="likesectionHead"><a 
 id="x1-40003"></a>References</h3>
<!--l. 1--><p class="noindent" >
   <div class="thebibliography">
   <p class="bibitem" ><span class="biblabel">
 [1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XG.E.Hinton2006"></a>Hinton GE, Salakhutdinov RR.  Reducing the Dimensionality of Data with Neural Networks.  In:
   Science; 2006. .
                                                                                         
                                                                                         
   </p>
   <p class="bibitem" ><span class="biblabel">
 [2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xautoencoder_visualization"></a>et&#x00A0;al  N.  A  dynamic  programming  approach  to  missing  data  estimation  using  neural  networks;.
   Available from: <a 
href="https://www.researchgate.net/figure/222834127_fig1" class="url" ><span 
class="cmtt-10">https://www.researchgate.net/figure/222834127_fig1</span></a>.
   </p>
   <p class="bibitem" ><span class="biblabel">
 [3]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XRumelhrt1986"></a>Rumelhrt,   Hinton,   Williams.       Learning   internal   representations   by   error   propogation.
   Parallel  distributed  processing:;  explorations  in  the  microstructure  of  congition;Available  from:
   <a 
href="http://dl.acm.org/citation.cfm?id=104293" class="url" ><span 
class="cmtt-10">http://dl.acm.org/citation.cfm?id=104293</span></a>.
   </p>
   <p class="bibitem" ><span class="biblabel">
 [4]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XBaldi1989"></a>Baldi, Hornik. Neural Networks and Principal Component Analysis: Learning from Examples Without
   Local Minima;. Available from: <a 
href="https://www.researchgate.net/publication/222438485" class="url" ><span 
class="cmtt-10">https://www.researchgate.net/publication/222438485</span></a>.
   </p>
   <p class="bibitem" ><span class="biblabel">
 [5]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="XHinton1994"></a>Hinton, Zemel. Autoencoders, Minimum Description Length, and Helmholtz Free Energy. Advances
   in             Neural             Information             Processsing             Systems;Available             from:
   <a 
href="https://www.cs.toronto.edu/~hinton/absps/cvq.pdf" class="url" ><span 
class="cmtt-10">https://www.cs.toronto.edu/</span><span 
class="cmtt-10">~</span><span 
class="cmtt-10">hinton/absps/cvq.pdf</span></a>.
</p>
   </div>
 
</body></html> 

                                                                                         


