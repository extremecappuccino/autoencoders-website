#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[hidelinks]{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Denoising Autoencoders
\end_layout

\begin_layout Standard
The principle behind denoising autoencoders is to be able to reconstruct
 data from an input of corrupted data.
 After giving the autoencoder the corrupted data, we force the hidden layer
 to learn only the more robust features, rather than just the identity.
 The output will then be a more refined version of the input data 
\begin_inset CommandInset citation
LatexCommand cite
key "deeplearning"

\end_inset

.
\end_layout

\begin_layout Standard
We can train a denoising autoencoder by stochastically corrupting data sets
 and inputting them into a neural network.
 The autoencoder can then be trained against the original data.
 One way to corrupt the data would be simply to randomly remove some parts
 of the data, so that the autoencoder is trying to predict the missing input.
\end_layout

\begin_layout Standard
An example of this is shown below, with the image an autoencoder is trained
 on being on the left, and a reconstruction of the middle image on the right.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename res/denoising-example.png
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Denoising autoencoder example on handwritten digits 
\begin_inset CommandInset citation
LatexCommand cite
key "opendeep"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Stacked denoising autoencoders are also useful for deep learning.
 A stacked denoising autoencoder is a denoising autoencoder with multiple
 hidden layers, and is trained layer by layer, by trying to minimize the
 error in the layer's reconstruction compared to it's input (the previous
 layer's output).
 By using a stacked autoencoder in an unsupervised manner, we can learn
 starting weights prior to the main supervised training procedure with deep
 learning.
 A regularization effect is then created as a result of the pretraining
 procedure creating an initialization point, to which the parameters are
 restricted to a near optimal local minimum 
\begin_inset CommandInset citation
LatexCommand cite
key "ERHAN2010"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename res/pretraining-deep-networks.png
	width 60text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Pretraining example visualization 
\begin_inset CommandInset citation
LatexCommand cite
key "ERHAN2010"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The above image, is a 2D visualization generated with ISOMAP of the functions
 represented by 50 networks with and 50 networks without pretraining, for
 supervised training over MNIST (handwritten digits database) with 2 hidden
 layers.
 The colours from deep blue to cyan represent the progression in iterations.
 As the image shows, the pretraining models all start in a small region
 of space and decrease in variance, towards a basin of attraction.
 The training was also quicker by pretraining the functions.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "denoising"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
