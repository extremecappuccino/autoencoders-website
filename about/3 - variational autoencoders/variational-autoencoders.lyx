#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard

\series bold
3 Variational Autoencoders
\end_layout

\begin_layout Standard
A variational autoencoder (VAE) is a neural network consisting of an encoder,
 a decoder and a loss function.
 They let us design generative models of data and fit them to large datasets
 and can also be used for image generation and reinforcement learning.
 They are a recent advancement in machine learning, having only been defined
 in 2013.
\end_layout

\begin_layout Standard
The first layer of the VAE is the encoder which will take the input and
 convert it into a latent vector.
 This can be done by reducing the mean squared error of the input and output,
 like a standard autoencoder.
 With images for example, we can now represent something like a picture
 of a cat as the vector [1.9, 8.2, 2.1].
\end_layout

\begin_layout Standard
However to make the VAE a generative model, we must add a constraint on
 the encoding network that forces it to generate latent vectors that roughly
 follow a gaussian distribution.
 This is the key feature of variational autoencoders.
 We must now make the network's latent variables match the unit gaussian
 distribution as closely as possible.
 This is done by changing our loss term to be the sum of the mean squared
 error and a latent loss.
 The mean squared error as usual, allows us to measure how accurately the
 network reconstructs images.
 The latent loss however, is the Kullback-Liebler divergence (KL divergence),
 which can measure how closely the variables match a unit gaussian distribution.
 The encoder can now be changed to generate a vector of means and a vector
 of standard deviations, rather than a vector of real variables.
 From this the KL divergence can be calculated.
 
\end_layout

\begin_layout Standard
By training the VAE to increase the standard deviation of the encoded data
 allows us to distinguish between data, such as images of faces, where there
 may normally be noise in the data creating uncertainity.
\end_layout

\end_body
\end_document
