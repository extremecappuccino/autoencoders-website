#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard

\series bold
3 Sparse Autoencoders
\end_layout

\begin_layout Standard
While autoencoders normally discover useful structures by having a small
 number of hidden units, they can also be useful with a large number of
 hidden units.
 By doing so, the autoencoder enlarges the given input's representation.
 This is possible by introducing a sparsity constraint.
 The aim of this, is to cause the large number of neurons to all have a
 low average output so that the neurons are inactive most of the time.
 If we are using a sigmoid activation function we would want their output
 to be as close to 0 as possible, and as close to -1 as possible if using
 a tanh activation function.
\end_layout

\begin_layout Standard
If we have the activation function of a neuron, 
\begin_inset Formula $a_{j}$
\end_inset

, we can calculate the average activation function of all the neurons, 
\begin_inset Formula $p_{j},$
\end_inset

 with the formula
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p_{j}=\frac{1}{m}\sum_{i=1}^{m}\left[a_{j}x\right]
\]

\end_inset


\end_layout

\begin_layout Standard
The aim of the sparsity constrapint is to minimise 
\begin_inset Formula $p_{j}$
\end_inset

, so that 
\begin_inset Formula $p_{j}=p_{c}$
\end_inset

 where 
\begin_inset Formula $p_{c}$
\end_inset

 is a small number close to 0 (for the sigmoid activation function), such
 as 0.05.
 We can do so by adding a penalty term to mean squared error cost function
 that we normally try to minimise for classical autoencoders.
 The penalty term, as with variational autoencoders, is the KL divergence
 between the Bernoulli random variables 
\begin_inset Formula $p_{j}$
\end_inset

and 
\begin_inset Formula $p_{c}$
\end_inset

, and can be calculated with the formula
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
KL(p_{j}||p_{c})=plog\frac{p_{c}}{p_{j}}+(1-p)log\frac{1-p_{c}}{1-p_{j}}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
e
\begin_inset Graphics
	filename KL_divergence.png
	lyxscale 70
	scale 70
	scaleBeforeRotation

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
KL divergence penalty term for 
\begin_inset Formula $p_{c}$
\end_inset

= 0.2.
 Credit: 
\begin_inset CommandInset citation
LatexCommand cite
key "CS294A_Lecture_notes"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
An advancement to sparse autoencoders is the k-sparse autoencoder.
 This is where we choose k neurons with the highest activation functions
 and ignore the others, by either sorting the activites or using ReLU activation
 functions and adaptively adjusting the thresholds until the k largest neurons
 are identified.
 This lets us tune the value of k to obtain a sparsity level most suitable
 for our dataset.
 A large sparsity level would learn very local features which may not be
 useful for identifying handwritten digits but useful for pre-training neural
 nets.
 
\begin_inset CommandInset citation
LatexCommand cite
key "AlirezaMakhzani2013"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename k-sparse_autoencoder.png
	lyxscale 80
	scale 80

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Different sparsity levels k, learnt from MNIST with 1000 hidden units.
 Credit: 
\begin_inset CommandInset citation
LatexCommand cite
key "AlirezaMakhzani2013"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "sparse_autoencoders"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
