#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "" "default"
\font_sans "" "default"
\font_typewriter "" "default"
\font_math "" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Recursive Autoencoders
\end_layout

\begin_layout Author
Jordan Spooner
\end_layout

\begin_layout Standard
As we know, autoencoders learn a (usually reduced dimensional) representation
 of their inputs.
 On this page, we will discuss how recursive autoencoders can take a sequence
 of representation vectors, and return a useful (reduced dimensional) representa
tion for that sequence.
\end_layout

\begin_layout Paragraph
Vector Representations for Sequences
\end_layout

\begin_layout Standard
Suppose we have a matrix 
\begin_inset Formula $\mathbf{L}$
\end_inset

 of representation vectors and an ordered sequence of 
\begin_inset Formula $m$
\end_inset

 elements, each with an index 
\begin_inset Formula $k$
\end_inset

 which is used to get the element's vector representation in 
\begin_inset Formula $\mathbf{L}$
\end_inset

.
 Our look-up operation is then a simple projection, using a binary vector
 
\begin_inset Formula $\mathbf{b}$
\end_inset

, with zero in every position except for the 
\begin_inset Formula $k$
\end_inset

th index:
\begin_inset Formula 
\[
\mathbf{x}_{i}=\mathbf{L}\mathbf{b}_{k}
\]

\end_inset


\end_layout

\begin_layout Standard
We can now express our ordered sequence of 
\begin_inset Formula $m$
\end_inset

 elements as a sequence of 
\begin_inset Formula $m$
\end_inset

 vector representations, namely 
\begin_inset Formula $\left(\mathbf{x}_{1},\dots,\mathbf{x}_{m}\right)$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Pollack"

\end_inset


\end_layout

\begin_layout Section
Unsupervised Recursive Autoencoders
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/rae.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The structure of a simple recursive autoencoder.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:rae"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Quote
See ***autoencoders.
\end_layout

\begin_layout Standard
We will now introduce a binary tree structure that allows for recursion,
 by introducing a number of hidden respresentations.
 Each parent node has two children.
 In the base case, both children are vector representations for two sequence
 elements, here we will use 
\begin_inset Formula $\mathbf{x}_{1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{x}_{2}$
\end_inset

.
 In every further case, we have one child as a hidden representation of
 the sequence so far, say 
\begin_inset Formula $\mathbf{y}_{j}$
\end_inset

, and the next 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 in our sequence to be processed.
 Such a tree structure is shown in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:rae"

\end_inset

.
 It should be noted that every hidden representation will have the same
 dimensionality as the elements of the sequence, here called 
\begin_inset Formula $n$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Tan"

\end_inset


\end_layout

\begin_layout Standard
In order to compute a parent representation 
\begin_inset Formula $\mathbf{p}$
\end_inset

, we consider its two children 
\begin_inset Formula $\mathbf{c}_{1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{c}_{2}$
\end_inset

:
\begin_inset Formula 
\[
\mathbf{p}=f\left(\mathbf{W}\left[\mathbf{c}_{1};\mathbf{c}_{2}\right]+\mathbf{b}\right)
\]

\end_inset

Here we multiply a matrix of parameters 
\begin_inset Formula $\mathbf{W}\in\mathbb{R}^{n\times2n}$
\end_inset

 by the concatenation of the two children and add a bias term 
\begin_inset Formula $b$
\end_inset

.
 We finally apply an element-wise activation function 
\begin_inset Formula $f$
\end_inset

, usually a sigmoidal function such as 
\begin_inset Formula $\tanh$
\end_inset

.
\end_layout

\begin_layout Standard
We then attempt to reconstruct the children from the parent vector in a
 reconstruction layer:
\begin_inset Formula 
\[
\left[\mathbf{c}_{1}';\mathbf{c}_{2}'\right]=\mathbf{W}'\mathbf{p}+\mathbf{b}'
\]

\end_inset


\end_layout

\begin_layout Standard
We can then calculate the reconstruction error by simply calculating the
 Euclidean distance between the children and their reconstruction:
\begin_inset Formula 
\[
E=0.5\left\Vert \left[\mathbf{c}_{1};\mathbf{c}_{2}\right]-\left[\mathbf{c}_{1}';\mathbf{c}_{2}'\right]\right\Vert ^{2}
\]

\end_inset

We then ***backpropogate the error as usual, finding a representation that
 encodes the children as well as possible.
\end_layout

\begin_layout Standard
After computing the reconstruction 
\begin_inset Formula $\left[\mathbf{c}_{1}';\mathbf{c}_{2}'\right]$
\end_inset

, if one of the children (say 
\begin_inset Formula $\mathbf{c}_{2}'$
\end_inset

) is a non-terminal node (some 
\begin_inset Formula $\mathbf{y}_{i}$
\end_inset

), we can again compute the reconstruction error of that 
\begin_inset Formula $\mathbf{y}_{i}$
\end_inset

 using the reconstruction 
\begin_inset Formula $\mathbf{c}_{2}'$
\end_inset

.
\end_layout

\begin_layout Standard
This process is repeated for each parent node until we have constructed
 the full tree, and calculated the reconstuction error for each element
 in the original sequence.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2011"

\end_inset


\end_layout

\begin_layout Subsection
Optimisations
\end_layout

\begin_layout Standard
There are several adjustments we can make to optimise recursive autoencoders:
\end_layout

\begin_layout Enumerate

\emph on
Choosing a good tree structure
\emph default
: Note that there are many ways of building the binary tree structure.
 One method of determining a good tree structure is to try a greedy algorithm
 that tests each possibility at each step, and chooses the children nodes
 that give the lowest reconstruction error.
\end_layout

\begin_layout Enumerate

\emph on
Choosing a good reconstruction error
\emph default
: We want our reconstruction error to penalise any representation that looses
 the meaning of the sequence.
 As such, it may be a good idea to penalise more heavily a reconstruction
 error on a node that encodes lots of children (i.e.
 many elements of the sequence), than a reconstruction error on a node that
 represents fewer children.
 We can implement this easily, by altering 
\begin_inset Formula $E$
\end_inset

 to take into account the number of elements 
\begin_inset Formula $n_{1}$
\end_inset

 and 
\begin_inset Formula $n_{2}$
\end_inset

 underneath the child nodes 
\begin_inset Formula $\mathbf{c}_{1}$
\end_inset

 and 
\begin_inset Formula $\mathbf{c}_{2}$
\end_inset

 respectively:
\begin_inset Formula 
\[
E=\frac{n_{1}}{n_{1}+n_{2}}\left\Vert \mathbf{c}_{1}-\mathbf{c}_{1}'\right\Vert ^{2}+\frac{n_{2}}{n_{1}+n_{2}}\left\Vert \mathbf{c}_{2}-\mathbf{c}_{2}'\right\Vert ^{2}
\]

\end_inset


\end_layout

\begin_layout Enumerate

\emph on
Length normalisation
\emph default
: Recursive autoencoders compute the hidden representations that they then
 reconstruct.
 In order to minimise the reconstruction error, the RAE may compute a hidden
 representation which is very small in magnitude, which will decrease the
 reconstruction error, but is clearly undesirable as we will lose the meaning
 of the sequence.
 We can alleviate this effect by forcing the length of each node to have
 length 1, through setting 
\begin_inset Formula $\mathbf{p}=\mathbf{p}/\left\Vert \mathbf{p}\right\Vert $
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Li2013"

\end_inset


\end_layout

\begin_layout Standard
The following page discusses how recursive autoencoders can be applied to
 the problem of sentiment analysis.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "rae"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
