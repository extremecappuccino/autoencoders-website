#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "" "default"
\font_sans "" "default"
\font_typewriter "" "default"
\font_math "" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Bilingual Representations
\end_layout

\begin_layout Author
Jordan Spooner
\end_layout

\begin_layout Standard
We will discuss another method for learning representations of phrases (namely
 the bag-of-words approach), and see how this can be applied to bilingual
 data where sentences are aligned.
\end_layout

\begin_layout Section
Bag-of-Words
\begin_inset CommandInset label
LatexCommand label
name "sec:bow"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/bow.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Demonstration of the bag-of-words approach and how a bag-of-words can be
 used to generate a vector representation for a phrase.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We assume a method for representing words in a vector in 
\begin_inset Formula $\mathbb{R}^{D}$
\end_inset

 exists.
 Methods for achieving this are discussed on other pages.
 We also assume a vocabulary of words, size 
\begin_inset Formula $V$
\end_inset

.
\end_layout

\begin_layout Standard
Given a sentence or phrase, we produce a representation by a simple additive
 method, which is as follows:
\end_layout

\begin_layout Enumerate
For each word in our vocabulary, we calculate its vector representation
 in 
\begin_inset Formula $\mathbb{R}^{D}$
\end_inset

.
 We then define a matrix, say 
\begin_inset Formula $\mathbf{W}$
\end_inset

 of dimensions 
\begin_inset Formula $D\times V$
\end_inset

.
 The matrix 
\begin_inset Formula $\mathbf{W}$
\end_inset

 simply has the vector representation for each of the 
\begin_inset Formula $V$
\end_inset

 vocabulary words as its columns.
\end_layout

\begin_layout Enumerate
We start with a bag-of-words representation 
\begin_inset Formula $\mathbf{x}$
\end_inset

, such that each element in 
\begin_inset Formula $\mathbf{x}$
\end_inset

 is an index referencing a word in the vocabulary of 
\begin_inset Formula $V$
\end_inset

 words.
 Note that in a bag-of-words representation, the order of words in the sentence
 is completely ignored.
\end_layout

\begin_layout Enumerate
The encoder function sums over each colum of 
\begin_inset Formula $\mathbf{W}$
\end_inset

 referring to a word in the given sentence or phrase, giving a result, which
 we will call 
\begin_inset Formula $\phi\left(\mathbf{x}\right)$
\end_inset

.
 The decoder function is trained using a loss function that measures how
 well we are able to reconstruct 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Fergus"

\end_inset


\end_layout

\begin_layout Paragraph
Concrete Implementation
\end_layout

\begin_layout Standard
In practice, there are many ways to implement a bag-of-words approach to
 learning phrase representations using autoencoders.
 We will now discuss a concrete example:
\end_layout

\begin_layout Enumerate
Using the bag-of-words representation 
\begin_inset Formula $\mathbf{x}$
\end_inset

, we define 
\begin_inset Formula $\mathbf{v}$
\end_inset

 in 
\begin_inset Formula $\mathbb{R}^{V}$
\end_inset

, whereby 
\begin_inset Formula $\mathbf{v}$
\end_inset

 is simply a binary vector, such that each 
\begin_inset Formula $v_{i}$
\end_inset

 is 1 if the phrase contains the 
\begin_inset Formula $i$
\end_inset

th word of the vocabulary, or 0 otherwise.
\end_layout

\begin_layout Enumerate
It is now simple to find the encoder representation, 
\begin_inset Formula $\phi\left(\mathbf{x}\right)$
\end_inset

 by multiplication of 
\series bold

\begin_inset Formula $\mathbf{v}$
\end_inset


\series default
 with 
\begin_inset Formula $\mathbf{W}$
\end_inset

:
\begin_inset Formula 
\[
\phi\left(\mathbf{x}\right)=f\left(\mathbf{Wv}+\mathbf{b}\right)
\]

\end_inset

where 
\begin_inset Formula $\mathbf{b}$
\end_inset

 is a bias vector and 
\begin_inset Formula $f$
\end_inset

 is a sigmoidal function, applied element wise to 
\begin_inset Formula $\mathbf{Wx}+\mathbf{b}$
\end_inset

.
\end_layout

\begin_layout Enumerate
The decoder attempts to reconstruct 
\begin_inset Formula $\mathbf{v}$
\end_inset

 from 
\begin_inset Formula $\phi\left(\mathbf{x}\right)$
\end_inset

:
\begin_inset Formula 
\[
\mathbf{\hat{v}}=g\left(\mathbf{W}'\phi\left(\mathbf{x}\right)+\mathbf{b}'\right)
\]

\end_inset


\end_layout

\begin_layout Enumerate
Training attempts to minimise the difference between 
\begin_inset Formula $\mathbf{v}$
\end_inset

 and its reconstruction.
 This is done by using a cross-entropy loss function.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Nielsen2017"

\end_inset


\end_layout

\begin_layout Section
Bilingual Autoencoders
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/bae.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Part of a bilingual autoencoder (BAE) that shows the process for calculating
 the second part of the loss function as described below.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Suppose we have a set of aligned 
\begin_inset Formula $\left(\mathbf{x},\mathbf{y}\right)$
\end_inset

 sentence pairs in two languages, 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Standard
We also assume vocabularies 
\begin_inset Formula $V_{X}$
\end_inset

 and 
\begin_inset Formula $V_{Y}$
\end_inset

 for each language, and 
\begin_inset Formula $\mathbf{W}_{X}$
\end_inset

 and 
\begin_inset Formula $\mathbf{W}_{Y}$
\end_inset

 of sizes 
\begin_inset Formula $D\times V_{X}$
\end_inset

 and 
\begin_inset Formula $D\times V_{Y}$
\end_inset

 respectively, as the word representation matrices as defined for a single
 language in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:bow"

\end_inset

.
\end_layout

\begin_layout Standard
The encoder, similarly to before, produces:
\begin_inset Formula 
\begin{eqnarray*}
\phi\left(\mathbf{x}\right) & = & f\left(\mathbf{W}_{X}\mathbf{v}\left(\mathbf{x}\right)+\mathbf{b}\right)\\
\phi\left(\mathbf{y}\right) & = & f\left(\mathbf{W}_{Y}\mathbf{v}\left(\mathbf{y}\right)+\mathbf{b}\right)
\end{eqnarray*}

\end_inset

where 
\begin_inset Formula $\mathbf{b}$
\end_inset

 is the same for both languages to keep the representations on roughly the
 same scale.
 Note that both have dimension 
\begin_inset Formula $D$
\end_inset

.
\end_layout

\begin_layout Standard
The decoders are of the same form as before, but with individual parameters
 for each language.
\end_layout

\begin_layout Paragraph
The Loss Function
\end_layout

\begin_layout Standard
This encoder-decoder structure gives us the benefit of being able to calculate
 five different reconstruction errors:
\end_layout

\begin_layout Enumerate
Reconstruction of 
\begin_inset Formula $\mathbf{y}$
\end_inset

 given input 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Reconstruction of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 given input 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Reconstruction of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 given input 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Reconstruction of 
\begin_inset Formula $\mathbf{y}$
\end_inset

 given input 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Reconstruction of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\series bold

\begin_inset Formula $\mathbf{y}$
\end_inset


\series default
 given inputs 
\begin_inset Formula $\mathbf{x}$
\end_inset

 and 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
\end_layout

\begin_layout Standard
For training, we can simply optimise a sum of these loss functions.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Ruder2012,Lauly,Chandar2014"

\end_inset


\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/bae-results.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Results achieved by Chandar Lauly et al using the methodology described
 above.
 The table shows six example words in English, along with the 10 closest
 words (by Euclidean distance) to each example, in both English and German.
 Source: 
\begin_inset CommandInset citation
LatexCommand cite
key "Chandar2014"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Quote
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "bilingual-words"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
