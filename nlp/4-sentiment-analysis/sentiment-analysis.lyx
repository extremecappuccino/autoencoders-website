#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
\usepackage{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Sentiment Analysis
\end_layout

\begin_layout Author
Jordan Spooner
\end_layout

\begin_layout Standard
This page continues from the previous page on recursive autoencoders.
\end_layout

\begin_layout Section
Phrase Representations using Recursive Autoencoders
\end_layout

\begin_layout Quotation
[Recursive Autoencoders]
\end_layout

\begin_layout Quote
// Recursive Autoencoder Diagram
\end_layout

\begin_layout Standard
We start with a phrase, and use a recursive autoencoder to produce a vector
 representation.
 This method follows naturally from the previous page, with the followin
 ammendments: our sequence of elements is now an ordered sequence of words.
 Concretely, we have the sequence 
\begin_inset Formula $\left(\mathbf{x}_{1},\dots,\mathbf{x}_{m}\right)$
\end_inset

 where each 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 is the vector representation of the 
\begin_inset Formula $i$
\end_inset

th word of a phrase made up of 
\begin_inset Formula $m$
\end_inset

 words.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Tan2014"

\end_inset


\end_layout

\begin_layout Section
Semi-Supervised Recursive Autoencoders
\end_layout

\begin_layout Quote
// Semi-Supervised Autoencoder Diagram
\end_layout

\begin_layout Standard
We now use our recursive autoencoder to attempt to predict a sentiment label.
 It is fairly easy to predict a probability distribution over the possible
 labels, by applying a simple softmax layer:
\family typewriter

\begin_inset Formula 
\[
\mathbf{d}\left(\mathbf{p}\right)=\mbox{softmax}\left(\mathbf{W}_{\mbox{label}}\mathbf{p}\right)
\]

\end_inset


\family default
where 
\begin_inset Formula $\mathbf{p}$
\end_inset

 is a given parent vector.
 The softmax layer applies a statistic method (specifically a generalization
 of logistic regression to handle more than two classes), to return the
 required probability distribution estimate, 
\begin_inset Formula $\mathbf{d}$
\end_inset

.
 Concretely, given 
\begin_inset Formula $K$
\end_inset

 labels, 
\begin_inset Formula $\mathbf{d}\in\mathbb{R}^{K}$
\end_inset

 and 
\begin_inset Formula $\sum_{k=1}^{K}\mathbf{d}_{k}=1$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Ng"

\end_inset


\end_layout

\begin_layout Standard
We also have a target distribution 
\begin_inset Formula $\mathbf{t}$
\end_inset

.
 This is the `correct' probability distribution, that we are aiming to learn.
 This means we are able to calculate a cross-entropy error, that will decrease
 as the predicted distribution 
\begin_inset Formula $\mathbf{d}$
\end_inset

 and the target distribution 
\begin_inset Formula $\mathbf{t}$
\end_inset

 become more similar.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Nielsen2017"

\end_inset


\end_layout

\begin_layout Standard
Note that we could have a simple case where 
\begin_inset Formula $K=2$
\end_inset

 and the set of labels includes only `good' and `bad'.
 This type of labelling is very common, making implementation easier.
 We could also have a significantly more complicated set of labels, depending
 on what training data sets are available.
\end_layout

\begin_layout Paragraph
Training
\end_layout

\begin_layout Standard
We start with a corpus of aligned (sentence, label) pairs 
\begin_inset Formula $\left(\mathbf{x},\mathbf{t}\right)$
\end_inset

.
 During training, we minimise a cost function which is constructed as follows:
\end_layout

\begin_layout Enumerate
We take into account an error for each entry in the training set, which
 is calculated from the sum over the errors at the non-terminal nodes of
 the tree constructed by the RAE:
\begin_inset Formula 
\[
\sum_{\left(\mathbf{x},\mathbf{t}\right)}\sum_{s\in\mbox{nodes}\left(\mbox{RAE}\left(x\right)\right)}\alpha E_{\mbox{rec}}\left(\left[\mathbf{c}_{1};\mathbf{c}_{2}\right]_{s}\right)+\left(1-\alpha\right)E_{\mbox{CE}}\left(\mathbf{p}_{s},\mathbf{t}\right)
\]

\end_inset

These errors in turn are calculated by adding the reconstruction error 
\begin_inset Formula $E_{\mbox{rec}}$
\end_inset

 (given on the previous page) to the cross entropy error 
\begin_inset Formula $E_{\mbox{CE}}$
\end_inset

 from the softmax regression layer.
 These two errors are weighted according to the value of the paramater 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Enumerate
We add to this the final reconstruction error for the full tree.
\end_layout

\begin_layout Quote
[Backpropogation]
\end_layout

\begin_layout Standard
Learning can then be achieved by backpropogation using this cost function.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2011"

\end_inset


\end_layout

\begin_layout Paragraph
Improvements
\end_layout

\begin_layout Standard
There are still better methods for sentiment analysis, which usually take
 into account the syntax tree of a given phrase or sentence.
 These methods, however, do not usually involve autoencoders! 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2013"

\end_inset


\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Quote
// Python Implementation
\end_layout

\begin_layout Quote
// Example Results
\end_layout

\begin_layout Standard
The following page discusses an extension of this model, allowing us to
 apply it to the problem of paraphrase detection.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "sentiment-analysis"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
