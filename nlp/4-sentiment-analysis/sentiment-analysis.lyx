#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "" "default"
\font_sans "" "default"
\font_typewriter "" "default"
\font_math "" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Sentiment Analysis
\end_layout

\begin_layout Author
Jordan Spooner
\end_layout

\begin_layout Standard
This page continues from the previous page on ***recursive autoencoders.
\end_layout

\begin_layout Section
Phrase Representations using Recursive Autoencoders
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/rae.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The structure of a simple recursive autoencoder.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We start with a phrase, and use a recursive autoencoder to produce a vector
 representation.
 This method follows naturally from the previous page, with the followin
 ammendments: our sequence of elements is now an ordered sequence of words.
 Concretely, we have the sequence 
\begin_inset Formula $\left(\mathbf{x}_{1},\dots,\mathbf{x}_{m}\right)$
\end_inset

 where each 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 is the vector representation of the 
\begin_inset Formula $i$
\end_inset

th word of a phrase made up of 
\begin_inset Formula $m$
\end_inset

 words.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Tan2014"

\end_inset


\end_layout

\begin_layout Section
Semi-Supervised Recursive Autoencoders
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/semi-supervised-rae.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
A semi-supervised recursive autoencoder.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We now use our recursive autoencoder to attempt to predict a sentiment label.
 It is fairly easy to predict a probability distribution over the possible
 labels, by applying a simple softmax layer:
\family typewriter

\begin_inset Formula 
\[
\mathbf{d}\left(\mathbf{p}\right)=\mbox{softmax}\left(\mathbf{W}_{\mbox{label}}\mathbf{p}\right)
\]

\end_inset


\family default
where 
\begin_inset Formula $\mathbf{p}$
\end_inset

 is a given parent vector.
 The softmax layer applies a statistic method (specifically a generalization
 of logistic regression to handle more than two classes), to return the
 required probability distribution estimate, 
\begin_inset Formula $\mathbf{d}$
\end_inset

.
 Concretely, given 
\begin_inset Formula $K$
\end_inset

 labels, 
\begin_inset Formula $\mathbf{d}\in\mathbb{R}^{K}$
\end_inset

 and 
\begin_inset Formula $\sum_{k=1}^{K}\mathbf{d}_{k}=1$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Ng"

\end_inset


\end_layout

\begin_layout Standard
We also have a target distribution 
\begin_inset Formula $\mathbf{t}$
\end_inset

.
 This is the `correct' probability distribution, that we are aiming to learn.
 This means we are able to calculate a cross-entropy error, that will decrease
 as the predicted distribution 
\begin_inset Formula $\mathbf{d}$
\end_inset

 and the target distribution 
\begin_inset Formula $\mathbf{t}$
\end_inset

 become more similar.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Nielsen2017"

\end_inset


\end_layout

\begin_layout Standard
Note that we could have a simple case where 
\begin_inset Formula $K=2$
\end_inset

 and the set of labels includes only `good' and `bad'.
 This type of labelling is very common, making implementation easier.
 We could also have a significantly more complicated set of labels, depending
 on what training data sets are available.
\end_layout

\begin_layout Paragraph
Training
\end_layout

\begin_layout Standard
We start with a corpus of aligned (sentence, label) pairs 
\begin_inset Formula $\left(\mathbf{x},\mathbf{t}\right)$
\end_inset

.
 During training, we minimise a cost function which is constructed as follows:
\end_layout

\begin_layout Enumerate
We take into account an error for each entry in the training set, which
 is calculated from the sum over the errors at the non-terminal nodes of
 the tree constructed by the RAE:
\begin_inset Formula 
\[
\sum_{\left(\mathbf{x},\mathbf{t}\right)}\sum_{s\in\mbox{nodes}\left(\mbox{RAE}\left(x\right)\right)}\alpha E_{\mbox{rec}}\left(\left[\mathbf{c}_{1};\mathbf{c}_{2}\right]_{s}\right)+\left(1-\alpha\right)E_{\mbox{CE}}\left(\mathbf{p}_{s},\mathbf{t}\right)
\]

\end_inset

These errors in turn are calculated by adding the reconstruction error 
\begin_inset Formula $E_{\mbox{rec}}$
\end_inset

 (given on the previous page) to the cross entropy error 
\begin_inset Formula $E_{\mbox{CE}}$
\end_inset

 from the softmax regression layer.
 These two errors are weighted according to the value of the paramater 
\begin_inset Formula $\alpha$
\end_inset

.
\end_layout

\begin_layout Enumerate
We add to this the final reconstruction error for the full tree.
\end_layout

\begin_layout Standard
Learning can then be achieved by ***backpropogation using this cost function.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2011"

\end_inset


\end_layout

\begin_layout Paragraph
Improvements
\end_layout

\begin_layout Standard
There are still better methods for sentiment analysis, which usually take
 into account the syntax tree of a given phrase or sentence.
 These methods, however, do not usually involve autoencoders! 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2013"

\end_inset


\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/sent-analysis.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The results achieved by Socher et al using this implementation.
 The red bars show the actual distribution from an online experiment, the
 light blue bars show the predicted probabilities.
 The five choices were (Sorry, hugs), (You rock), (Teehee), (I understand)
 and (Wow, just wow), in that order.
 Source: 
\begin_inset CommandInset citation
LatexCommand cite
key "Socher2011"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
A Java implementation of this method can be found at 
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/sancha/jrae"

\end_inset

.
\end_layout

\begin_layout Standard
The following page discusses an extension of this model, allowing us to
 apply it to the problem of ***paraphrase detection.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "sentiment-analysis"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
