#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "" "default""
\font_sans "" "default""
\font_typewriter "" "default""
\font_math "" "auto""
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Word and Phrase Representations
\end_layout

\begin_layout Author
Jordan Spooner
\end_layout

\begin_layout Standard
On this page, we will discuss how autoencoders can learn vector representations
 for phrases, as proposed by Lebret and Collobert in 2015.
 In order to do so, we will need to discuss first how to find vector representat
ions for single words.
\end_layout

\begin_layout Section
Word Embeddings
\end_layout

\begin_layout Standard
A common approach to learning word representations is to consider a word's
 most common neighbours.
\end_layout

\begin_layout Subsection
Word Co-occurrence Probabilities
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/corpus.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
An example corpus and demonstration of selection of context words.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We start with a corpus 
\begin_inset Formula $W$
\end_inset

 (a large and structured set of texts).
 We then define a context 
\begin_inset Formula $D$
\end_inset

, which determines exactly what requirements need to be met in order for
 a word to be considered a `neighbour' 
\begin_inset Formula $c$
\end_inset

 to any selected word 
\begin_inset Formula $w$
\end_inset

 .
 We will here define a context as the six words surrounding (three preceeding
 and three following) the currently selected word (although it could be
 defined differently to this).
 We can the calculate the co-occurrence probabilites as follows:
\begin_inset Formula 
\[
p\left(c|w\right)=\frac{p\left(c,w\right)}{p\left(w\right)}=\frac{n\left(c,w\right)}{\sum_{c_{j}\in D}n\left(c_{j},w\right)}
\]

\end_inset

Here, 
\begin_inset Formula $n\left(c,w\right)$
\end_inset

 gives the number of times that 
\begin_inset Formula $c$
\end_inset

 occurs in the context of 
\begin_inset Formula $w$
\end_inset

.
 
\end_layout

\begin_layout Standard
For each word 
\begin_inset Formula $w$
\end_inset

 in the corpus 
\begin_inset Formula $W$
\end_inset

, we then end up with a probability distribution, 
\begin_inset Formula $P_{w}=\left(p\left(c_{1}|w\right),\dots,p\left(c_{\left|D\right|}|w\right)\right)$
\end_inset

 that gives the probability of the co-occurence each context word 
\begin_inset Formula $c$
\end_inset

 in the set of context words 
\begin_inset Formula $D$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Khaz2016,Web1"

\end_inset


\end_layout

\begin_layout Standard
For example, consider a very simple corpus that consists of the following
 four texts:
\end_layout

\begin_layout Quotation
\begin_inset Quotes eld
\end_inset

Projects which are open-source, and developed collaboratively by diverse
 communities, generate an increasingly more diverse scope of design perspective
 than any one company is capable of developing.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Quotation
\begin_inset Quotes eld
\end_inset

If a program is open-source, its source code is freely available to its
 users.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Quotation
\begin_inset Quotes eld
\end_inset

When an author contributes code to open-source projects, they do so under
 an explicit license or an implicit license.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Quotation
\begin_inset Quotes eld
\end_inset

Contributing to open-source software is an excellent way to improve your
 programming skills.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
Let us select the word 
\emph on
open-source
\emph default
 as 
\begin_inset Formula $w$
\end_inset

.
 We then get the following counts 
\begin_inset Formula $n\left(c,w\right)$
\end_inset

:
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="20" columns="2">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Context word 
\begin_inset Formula $c$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Count 
\begin_inset Formula $n\left(c,w\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
projects
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
which
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
are
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
and
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
developed
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
collaboratively
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
a
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
program
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
is
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
its
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
source
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
code
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
contributes
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
to
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
they
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
do
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
contributing
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
software
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
an
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
You can see how, as we increased the size of the corpus by finding more
 and longer relevant texts, we would begin to associate the word 
\emph on
open-source
\emph default
 strongly with words such as 
\emph on
project
\emph default
, 
\emph on
developed
\emph default
, 
\emph on
program
\emph default
, 
\emph on
code
\emph default
, 
\emph on
contributes
\emph default
, 
\emph on
software
\emph default
 and so on.
 We would see similar counts for similar words.
 For example, a vector generated from the co-occurence probabilities of
 the word 
\emph on
charitable
\emph default
 might be close because of surrounding context words such as 
\emph on
free
\emph default
 and 
\emph on
contributing
\emph default
.
 A word such as 
\emph on
programming
\emph default
 would be considered close due to words such as 
\emph on
code
\emph default
 and 
\emph on
project
\emph default
.
 We can determine this `closeness' of words using the Hellinger distance.
\end_layout

\begin_layout Subsection
Hellinger Distance
\end_layout

\begin_layout Standard
The Hellinger distance is a measure of the similarity of two probability
 distributions.
 Given the two discrete probability distributions 
\begin_inset Formula $\mathbf{p}=\left(p_{1},\dots,p_{k}\right)$
\end_inset

 and 
\begin_inset Formula $\mathbf{q}=\left(q_{1},\dots,q_{k}\right)$
\end_inset

, the Hellinger distance is defined as:
\begin_inset Formula 
\[
H\left(\mathbf{p},\mathbf{q}\right)=\frac{1}{\sqrt{2}}\sqrt{\sum_{i=1}^{k}\left(\sqrt{p_{i}}-\sqrt{q_{i}}\right)^{2}}
\]

\end_inset

Note that the Hellinger distance will always take a value between 0 and
 1, due the the constant factor of 
\begin_inset Formula $1/\sqrt{2}$
\end_inset

.
 This is one property of the Hellinger distance which makes it preferable
 to the standard Euclidean distance for this purpose.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Web1"

\end_inset


\end_layout

\begin_layout Subsection
Generating Word Vectors
\end_layout

\begin_layout Standard
The model we have so far is fine, except for the fact that each probabibility
 distribution 
\begin_inset Formula $\mathbf{p}_{w}$
\end_inset

 will generate a vector containing hundreds of thousands of entries (potentially
 as large as the number of words in 
\begin_inset Formula $W$
\end_inset

).
 A common approach to reducing the dimensionality of this vector (generally
 to 50 or 100 dimensions) is to consider a co-occurence matrix that only
 includes the most relevant context words, and use principal component analysis
 (PCA) to reduce dimensionality without losing too much meaning.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Web1"

\end_inset

 The use of autoencoders is an alternative method.
\end_layout

\begin_layout Paragraph
An Approach using Autoencoders
\end_layout

\begin_layout Standard
As the input, we provide an ***autoencoder with a probability distribution
 
\begin_inset Formula $\sqrt{\mathbf{p}_{w}}$
\end_inset

.
 We use a standard squared difference cost function:
\begin_inset Formula 
\begin{equation}
\left\Vert g\left(f\left(\sqrt{\mathbf{p}_{w}}\right)\right)-\sqrt{\mathbf{p}_{w}}\right\Vert ^{2}\label{Cost1}
\end{equation}

\end_inset

The autoencoder will learn a more compact (reduced dimensionality) representatio
n for this distribution, which can then be compared with other distributions
 using the Hellinger distance.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Lebret2015"

\end_inset


\end_layout

\begin_layout Section
Simple Phrase Representations
\end_layout

\begin_layout Standard
We will now discuss a very simple method for finding vector representations
 for phrases.
 There are many methods for doing so, most of which perform better than
 the method we discuss here.
\end_layout

\begin_layout Standard
To determine representations for phrases, we simply add word representations.
 This will work for simple phrases such as 
\begin_inset Quotes eld
\end_inset

the red cat
\begin_inset Quotes erd
\end_inset

, where the meaning is simply a sum of its parts.
 It does not work for more complex phrases (such as those involving negation).
\end_layout

\begin_layout Standard
We start with a phrase 
\begin_inset Formula $p=\left(w_{1},w_{2},\dots,w_{T}\right)$
\end_inset

 of 
\begin_inset Formula $T$
\end_inset

 words.
 We simple feed 
\begin_inset Formula $\sqrt{\mathbf{p}_{w}}$
\end_inset

 into the autoencoder for each word 
\begin_inset Formula $w$
\end_inset

 in the phrase 
\begin_inset Formula $p$
\end_inset

, which returns for each word, the vector representation 
\begin_inset Formula $\mathbf{x}_{w}=f\left(\sqrt{\mathbf{p}_{w}}\right)$
\end_inset

.
 The very simple process of element-wise addition gives us the representation
 of the phrase 
\begin_inset Formula $p$
\end_inset

 as:
\begin_inset Formula 
\[
\mathbf{x}_{p}=\frac{1}{T}\sum_{i=1}^{T}\mathbf{x}_{w_{i}}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Training
\end_layout

\begin_layout Standard
We can improve our representation by a method of training which involves
 predicting whether words are in a given phrase or not.
 We use a ranking-type cost on each phrase:
\begin_inset Formula 
\begin{equation}
\sum_{w_{t}\in p}\sum_{w_{i}\in W}\max\left(0,1-\mathbf{x}_{s}\cdot\mathbf{x}_{w_{t}}+\mathbf{x}_{s}\cdot\mathbf{x}_{w_{i}}\right)\label{Cost2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Due to the large size of 
\begin_inset Formula $W$
\end_inset

, we might choose to use negative sampling to speed up the training process.
 This reduces the number of words we will need to consider from hundreds
 of thousands to somewhere to the order of ten.
 
\begin_inset CommandInset citation
LatexCommand cite
key "McCormick2017"

\end_inset


\end_layout

\begin_layout Paragraph
Joint Learning
\end_layout

\begin_layout Standard
The benefit of this method (autoencoders), is that we can learn word representat
ions that are optimal for summing, by jointly minimising the objective functions
 
\begin_inset CommandInset ref
LatexCommand ref
reference "Cost1"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "Cost2"

\end_inset

 over the training data.
\end_layout

\begin_layout Paragraph
Improving Phrase Representations
\end_layout

\begin_layout Quote
See ***Recursive Autoencoders, ***Sentiment Analysis and ***Paraphrase Detection
\end_layout

\begin_layout Standard
As we discussed earlier, this is a relitavely poor method for representing
 phrases.
 We discuss a much better method on the pages for sentiment analysis and
 paraphrase detection, which uses recursive autoencoders.
\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/results.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The results of Lebret and Collobert in their 2015 paper.
 This table gives five example phrases, and five of the ten closest phrases
 according to an implementation of the model described on this page.
 The first column gives the results when using a symmetric window of ten
 context words around the query phrase.
 The second column gives the results when using an average of the individual
 word vectors.
 All representations were of 100 dimensions.
 Source: 
\begin_inset CommandInset citation
LatexCommand cite
key "Lebret2015"

\end_inset

.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
An example implementation in Python, using TensorFlow can be found at 
\begin_inset CommandInset href
LatexCommand href
target "https://github.com/ParallelDots/WordEmbeddingAutoencoder"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "word-embeddings"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
