#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[hidelinks]{hyperref}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding global
\font_roman "" "default""
\font_sans "" "default""
\font_typewriter "" "default""
\font_math "" "auto""
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip medskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Using Autoencoders
\end_layout

\begin_layout Author
Laurence Squires
\end_layout

\begin_layout Section
Visualising Trained Autoencoders
\end_layout

\begin_layout Standard
Consider the case of a trained autoencoder on 20x20 black and white images,
 so the number of input neurons to the network is 400.
 The network has been trained to output the same image but has a constraint
 on the number of hidden neurons in order to force the image to be compressed
 to a more efficient internal representation.
 This would be useful for other applications as reducing the input parameters
 with minimal loss in reconstruction error (infomation loss) means that
 other systems have less redundant inputs.
 To gain a better understanding on the inner workings of the network we
 can try to visualize the input which maximizes any given hidden layer neuron.
 This will reveal exactly what kind of input each hidden neuron is responding
 too, and so how the network has learned to represent the input.
 We can now derive a formula for generating these images.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $a_{i}$
\end_inset

be the state of each hidden layer neuron, 
\begin_inset Formula $x_{j}$
\end_inset

the input neuron, 
\begin_inset Formula $W_{ij}$
\end_inset

the weight between input 
\begin_inset Formula $j$
\end_inset

 and hidden 
\begin_inset Formula $i$
\end_inset

, 
\begin_inset Formula $b_{i}$
\end_inset

the bias for hidden 
\begin_inset Formula $i$
\end_inset

 and 
\begin_inset Formula $f(x)$
\end_inset

 the chosen increasing activation function.
 Each hidden neuron computes its inner state to be:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align center

\size larger
\begin_inset Formula $a_{i}=$
\end_inset


\begin_inset Formula $f(\sum_{j=1}^{400}W_{ij}x_{j})+b_{i}$
\end_inset


\end_layout

\begin_layout Standard
Assuming the input to be constrained between -1 and 1 and ignoring the bias
 term we can see that letting
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align center

\size larger
\begin_inset Formula $x_{j}=\frac{W_{ij}}{\sqrt{\sum_{j=1}^{400}(W_{ij})^{2}}}$
\end_inset


\end_layout

\begin_layout Standard
Will lead to a highly activated function, as 
\begin_inset Formula $(W_{ij})^{2}$
\end_inset

 is guaranteed to be positive and dividing by the squared allows 
\begin_inset Formula $x_{j}$
\end_inset

to maintain the constraint between -1 and 1 and offers a useful comparison
 between inputs 
\begin_inset CommandInset citation
LatexCommand cite
key "visualizingStanford"

\end_inset

.
 We can then plot 
\begin_inset Formula $x_{j}$
\end_inset

(the input) for each hidden layer to show what the neuron responds to from
 the input.
 In this case on a network trained on images of natural images with 100
 hidden neurons can be visualized as this:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/ExampleSparseAutoencoderWeights.png
	lyxscale 70
	scale 70
	rotateOrigin centerTop

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Visualization of the hidden layers in an autoencoder
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Each square shows the input required to activate each hidden layer neuron,
 we can see that the network has trained to respond strongly to straight
 edges of different sizes and orientations 
\begin_inset CommandInset citation
LatexCommand cite
key "plottingMathworks"

\end_inset

.
 These features are not surprising as representing an image by a combination
 of edges is a useful representation for the real world.
 When trained on different images or other inputs (such as audio or some
 abstract input type) the network will also learn a useful representation
 like this.
 This kind of representation of images can be found in many other types
 of nerual networks, particulary convulation nerual networks as they are
 also tasked with processing natural image data and so develop similiar
 latent representations of the input image 
\begin_inset CommandInset citation
LatexCommand citet
key "karpathyStanford"

\end_inset

.
\end_layout

\begin_layout Section
Image In-painting
\end_layout

\begin_layout Standard
Image inpainting is the problem of restoring lost infomation from images,
 classicaly a black sqaure has been overlayed on a random region of the
 image causing permenant loss of infomation from the image.
 The objective of image inpaiting is to restore this infomation by 
\begin_inset Quotes eld
\end_inset

painting
\begin_inset Quotes erd
\end_inset

 the lost region back in, so not only does the region of image loss need
 to be identified it also needs to be restored by using infomation about
 the surrounding regions of the image.
 This problem has large scale applications in video and photo editing as
 well as compression and image processing 
\begin_inset CommandInset citation
LatexCommand cite
key "inpainting"

\end_inset

.
 
\end_layout

\begin_layout Standard
Autoencoders can be applied to solve this problem due to their abilities
 in image processing and unsupervised training.
 By stacking autoencorders into multiple layers (eg having 2 or more layers
 in the encoding section of the network) this allows the network to learn
 more abstract features and can be trained quickly by training the first
 layer as an autoencoder, and then training the second layer seperately
 before finally joing them together 
\begin_inset CommandInset citation
LatexCommand cite
key "inpaintingAutoEncoders"

\end_inset

.
\end_layout

\begin_layout Standard
The results of training on a 5 layer are shown below (right) with other
 techniques in the middle and the original and corrupted image on the left.
 The network has not only filled in the correct colour but has also re-establish
ed basic features such as the shape of the eye.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename imgs/inpainting.PNG
	rotateOrigin centerTop

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Inpainting results for portrait photos.
 From left to right, original image, corrupted image, iterative training
 (3 layers), normal training (3 layer), iterative training (5 layer), normal
 training (5 layer).
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "autoencoders"
options "vancouver"

\end_inset


\end_layout

\end_body
\end_document
